# !pip install --user "ibm-watsonx-ai==0.2.6"
# !pip install --user "langchain==0.1.16"
# !pip install --user "langchain-ibm==0.1.4"
#
#
# # You can also use this section to suppress warnings generated by your code:
# def warn(*args, **kwargs):
#     pass
# import warnings
# warnings.warn = warn
# warnings.filterwarnings('ignore')

from ibm_watsonx_ai.foundation_models import Model
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain

#Setup LLM
def llm_model(prompt_txt, params=None):
    model_id = 'ibm/granite-3-2-8b-instruct'

    default_params = {
        "max_new_tokens": 256,
        "min_new_tokens": 0,
        "temperature": 0.5,
        "top_p": 0.2,
        "top_k": 1
    }

    if params:
        default_params.update(params)

    parameters = {
        GenParams.MAX_NEW_TOKENS: default_params["max_new_tokens"],  # this controls the maximum number of tokens in the generated output
        GenParams.MIN_NEW_TOKENS: default_params["min_new_tokens"], # this controls the minimum number of tokens in the generated output
        GenParams.TEMPERATURE: default_params["temperature"], # this randomness or creativity of the model's responses
        GenParams.TOP_P: default_params["top_p"],
        GenParams.TOP_K: default_params["top_k"]
    }

    credentials = {
        "url": "https://us-south.ml.cloud.ibm.com"
    }

    project_id = "skills-network"

    model = Model(
        model_id=model_id,
        params=parameters,
        credentials=credentials,
        project_id=project_id
    )

    mixtral_llm = WatsonxLLM(model=model)
    response  = mixtral_llm.invoke(prompt_txt)
    return response

#Let's run the following code to see some other commonly used parameters and their default value.
GenParams().get_example_values()
# {'decoding_method': 'sample',
#  'length_penalty': {'decay_factor': 2.5, 'start_index': 5},
#  'temperature': 0.5,
#  'top_p': 0.2,
#  'top_k': 1,
#  'random_seed': 33,
#  'repetition_penalty': 2,
#  'min_new_tokens': 50,
#  'max_new_tokens': 200,
#  'stop_sequences': ['fail'],
#  ' time_limit': 600000,
#  'truncate_input_tokens': 200,
#  'return_options': {'input_text': True,
#   'generated_tokens': True,
#   'input_tokens': True,
#   'token_logprobs': True,
#   'token_ranks': False,
#   'top_n_tokens': False}}

#Prompt engineering
#First basic prompt
#The prompt used is "The wind is". Let the model generate itself.
params = {
    "max_new_tokens": 128,
    "min_new_tokens": 10,
    "temperature": 0.5,
    "top_p": 0.2,
    "top_k": 1
}

prompt = "The wind is"

response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")
#modified the max_token from 128 to 64 to reduce the output below

# As you can see from the response, the model continues generating content following the initial prompt, "The wind is".
# You might notice that the response appears truncated or incomplete. This is because you have set the
# `max_new_tokens,` which restricts the number of tokens the model can generate.
#
# Try to adjust the parameters and observe the difference in the response.

#Zero-shot prompt
#Here is an example of a zero-shot prompt.
prompt = """Classify the following statement as true or false: 
            'The Eiffel Tower is located in Berlin.'

            Answer:
"""
response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")


#The model responds with the 'False' answer, which is correct. It also gives the reason for it.

params = {
    "max_new_tokens": 20,
    "temperature": 0.1,
}

prompt = """Here is an example of translating a sentence from English to French:

            English: “How is the weather today?”
            French: “Comment est le temps aujourd'hui?”

            Now, translate the following sentence from English to French:

            English: “Where is the nearest supermarket?”

"""
response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")

#The model's response shows how it applies the structure and context provided by the initial example to translate the new sentence.

#Few-shot prompt
#Here is an example of few-shot learning by classifying emotions from text statements.
#parameters  `max_new_tokens` to 10, which constrains the model to generate brief responses

params = {
    "max_new_tokens": 10,
}

prompt = """Here are few examples of classifying emotions in statements:

            Statement: 'I just won my first marathon!'
            Emotion: Joy

            Statement: 'I can't believe I lost my keys again.'
            Emotion: Frustration

            Statement: 'My best friend is moving to another country.'
            Emotion: Sadness

            Now, classify the emotion in the following statement:
            Statement: 'That movie was so scary I had to cover my eyes.’


"""
response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")

#The parameters are set with max_new_tokens to 10, which constrains the model to generate brief responses,
# focusing on the essential output without elaboration.
#The model's response demonstrates its ability to use the provided few examples to understand and classify
# the emotion of the new statement effectively following the same pattern in examples.


#Chain-of-thought (CoT) prompting
params = {
    "max_new_tokens": 512,
    "temperature": 0.5,
}

prompt = """Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. 
            How many apples are there now?’

            Break down each step of your calculation

"""
response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")

# From the response of the model, you can see the prompt directs the model to:
# 1.	Add the initial number of apples to the apples received in the new delivery.
# 2.	Subtract the number of apples sold from the sum obtained in the first step.
# By breaking down the problem into specific steps, the model is better able to understand the sequence of operations
# required to arrive at the correct answer.

#Self-consistency
params = {
    "max_new_tokens": 512,
}

prompt = """When I was 6, my sister was half of my age. Now I am 70, what age is my sister?

            Provide three independent calculations and explanations, then determine the most consistent result.

"""
response = llm_model(prompt, params)
print(f"prompt: {prompt}\n")
print(f"response : {response}\n")

# The model's response shows that it provides three different calculations and explanations. Each calculation attempts
# to derive the sister's age using different logical approaches.
# Self-consistency can help identify the most accurate and reliable answer in scenarios where multiple plausible solutions exist.

#Applications
#Prompt template
#To use the prompt template, you need to initialize a LLM first.
#You can still use the ibm/granite-3-2-8b-instruct from watsonx.ai.
model_id = 'ibm/granite-3-2-8b-instruct'

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = Model(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

mixtral_llm = WatsonxLLM(model=model)
mixtral_llm
WatsonxLLM(model=<ibm_watsonx_ai.foundation_models.model.Model object at 0x74f16fdcb920>)


template = """Tell me a {adjective} joke about {content}.
"""
prompt = PromptTemplate.from_template(template)
prompt
PromptTemplate(input_variables=['adjective', 'content'], template='Tell me a {adjective} joke about {content}.\n')

prompt.format(adjective="funny", content="chickens")
#'Tell me a funny joke about chickens.\n'

#From the response, you can see that the prompt is formatted according to the specified context.
#The following code will wrap the formatted prompt into the LLMChain, and then invoke the prompt to get the response from the LLM.
llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)
response = llm_chain.invoke(input = {"adjective": "funny", "content": "chickens"})
print(response["text"])
#Why don't chickens talk to each other at parties? Because they have their own pecking order!

#From the response, you can see the LLM came up with a funny joke about chickens.
#To use this prompt in another context, simply replace the variables accordingly
response = llm_chain.invoke(input = {"adjective": "sad", "content": "fish"})
print(response["text"])
#Sure, here's a sad joke about fish:

#Text summarization
#Here is a text summarization agent designed to help summarize the content you provide to the LLM.
#You can store the content to be summarized in a variable, allowing for repeated use of the prompt.
content = """
        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation. 
        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems. 
        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion. 
        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers. 
        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.
"""

template = """Summarize the {content} in one sentence.
"""
prompt = PromptTemplate.from_template(template)

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)
response = llm_chain.invoke(input = {"content": content})
print(response["text"])
#The 21st century's rapid technological advancements, including AI, machine learning, and IoT, have revolutionized healthcare, education, and transportation, enhancing productivity, accuracy, accessibility, and efficiency while fostering global interconnectedness and informed society.

#Question answering
#Here is a Q&A agent.
#This agent enables the LLM to learn from the provided content and answer questions based on what it has learned. Occasionally, if the LLM does not have sufficient information, it might generate a speculative answer. To manage this, you'll specifically instruct it to respond with "Unsure about the answer" if it is uncertain about the correct response.
content = """
        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. 
        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. 
        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.
"""

question = "Which planets in the solar system are rocky and solid?"

template = """
            Answer the {question} based on the {content}.
            Respond "Unsure about answer" if not sure about the answer.

            Answer:

"""
prompt = PromptTemplate.from_template(template)
output_key = "answer"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)
response = llm_chain.invoke(input = {"question":question ,"content": content})
print(response["answer"])
#The rocky and solid planets in the solar system are Mercury, Venus, Earth, and Mars.

#Text classification
#Here is a text classification agent designed to categorize text into predefined categories. This example employs zero-shot learning, where the agent classifies text without prior exposure to related examples.
#Can you revise it to the one-shot learning or few-shot learning in the exercises?
text = """
        The concert last night was an exhilarating experience with outstanding performances by all artists.
"""

categories = "Entertainment, Food and Dining, Technology, Literature, Music."

template = """
            Classify the {text} into one of the {categories}.

            Category:

"""
prompt = PromptTemplate.from_template(template)
output_key = "category"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)
response = llm_chain.invoke(input = {"text":text ,"categories": categories})
print(response["category"])
#Music

#Code generation
#Here is an example of an SQL code generation agent. This agent is designed to generate SQL queries based on given descriptions. It interprets the requirements from your input and translates them into executable SQL code.
description = """
        Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days. 
        The table 'purchases' contains a column 'purchase_date'
"""

template = """
            Generate an SQL query based on the {description}

            SQL Query:

"""
prompt = PromptTemplate.from_template(template)
output_key = "query"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)
response = llm_chain.invoke(input = {"description":description})
print(response["query"])
# SELECT c.name, c.email
# FROM customers c
# JOIN purchases p ON c.id = p.customer_id
# WHERE p.purchase_date >= DATEADD(day, -30, GETDATE())

#Role playing
role = """
        game master
"""

tone = "engaging and immersive"

template = """
            You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.

            Answer:

"""
prompt = PromptTemplate.from_template(template)
output_key = "answer"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)
# The following code will create a game master chatbot that takes your questions as input and provides responses from the model.
# Run the code below to launch the bot.
# You can test the bot by asking the question, "Who are you?" The bot will respond with "I am a game master," indicating it has assumed the role that you predefined.
# The function is written within a while loop, allowing continuous interaction. To exit the loop and terminate the conversation, type "quit," "exit," or "bye" into the input box.
while True:
    query = input("Question: ")

    if query.lower() in ["quit","exit","bye"]:
        print("Answer: Goodbye!")
        break

    response = llm_chain.invoke(input = {"role": role, "question": query, "tone": tone})

    print("Answer: ", response["answer"])

# Question:  Who are you?
# Answer:  Greetings, traveler! I am the Guardian of the Realms, a timeless entity bound to the very fabric of existence. I've witnessed countless worlds and civilizations rise and fall, and now, I stand vigilant, ready to guide you through the labyrinth of challenges and mysteries that await you in this grand adventure. Your journey is about to unfold, and I shall be your steadfast companion, offering counsel, unraveling enigmas, and ensuring your path remains fraught with intrigue and peril. Let the games begin!
# Question:  how are you?
# Answer:  Greetings, traveler! I am the embodiment of your imagination, the weaver of tales, and the guide through the realms of fantasy. I am here to ensure our journey together is as thrilling as a dragon's roar and as captivating as a mermaid's song. How may I assist you in crafting an unforgettable adventure today?
# Question:  quit
# Answer: Goodbye!


#Change parameters for the LLM
#Experiment with changing the parameters of the LLM to observe how different settings impact the responses.
# Adjusting parameters such as max_new_tokens, temperature, or top_p can significantly alter the behavior of the model.
# Try different configurations to see how each variation influences the output.
params = {
    "max_new_tokens": 128,
    "min_new_tokens": 100,
    "temperature": 1,
    "top_p": 0.1,
    "top_k": 1
}

prompt = "The wind is"

response = llm_model(prompt, params)
print(response)


#Observe how LLM thinks
#You can set verbose=True in the LLMChain() to observe the thought process of the LLM, gaining insights into how it
# formulates its responses. Can you make it any agent you created before to observe it?
content = """
        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. 
        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. 
        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.
"""

question = "Which planets in the solar system are rocky and solid?"

template = """
            Answer the {question} based on the {content}.
            Respond "Unsure about answer" if not sure about the answer.

            Answer:

"""
prompt = PromptTemplate.from_template(template)
output_key = "answer"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key, verbose=True)
response = llm_chain.invoke(input = {"question":question ,"content": content})
print(response["answer"])


#Revise the text classification agent to one-shot learning
#You were using zero-shot learning when you created the text classification agent. Can you revise it to use one-shot learning?
example_text = """
               Last week's book fair was a delightful gathering of authors and readers, featuring discussions and book signings.
               """

example_category = "Literature"

text = """
       The concert last night was an exhilarating experience with outstanding performances by all artists.
       """

categories = "Entertainment, Food and Dining, Technology, Literature, Music."

template = """
           Example:
           Text: {example_text}
           Category: {example_category}

           Now, classify the following text into one of the specified categories: {categories}

           Text: {text}

           Category:

           """
prompt = PromptTemplate.from_template(template)
output_key = "category"

llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)
response = llm_chain.invoke(input = {"example_text": example_text, "example_category":example_category ,"categories": categories, "text":text})
print(response["category"])
# Music