# !pip install ibm-watsonx-ai==0.2.6
# !pip install langchain==0.1.16
# !pip install langchain-ibm==0.1.4
# !pip install transformers==4.41.2
# !pip install huggingface-hub==0.23.4
# !pip install sentence-transformers==2.5.1
# !pip install chromadb
# !pip install wget==3.2
# !pip install --upgrade torch --index-url https://download.pytorch.org/whl/cpu
#
# !pip list | grep langchain
#
# # You can use this section to suppress warnings generated by your code:
# def warn(*args, **kwargs):
#     pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

from ibm_watsonx_ai.foundation_models import Model
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes, DecodingMethods
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
import wget

#Preprocessing
#Load the document
filename = 'companyPolicies.txt'
url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'

# Use wget to download the file
wget.download(url, out=filename)
print('file downloaded')
with open(filename, 'r') as file:
    # Read the contents of the file
    contents = file.read()
    print(contents)



#Splitting the document into chunks
loader = TextLoader(filename)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
print(len(texts))


#From the ouput of print, you see that the document has been split into 16 chunks
#Embedding and storing
embeddings = HuggingFaceEmbeddings()
docsearch = Chroma.from_documents(texts, embeddings)  # store the embedding in docsearch using Chromadb
print('document ingested')

#Up to this point, you've been performing the `Indexing` task. The next step is the `Retrieval` task.

#LLM model construction
model_id = 'google/flan-t5-xl'
parameters = {
    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,
    GenParams.MIN_NEW_TOKENS: 130, # this controls the minimum number of tokens in the generated output
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses
}
#Define credentials and project_id, which are necessary parameters to successfully run LLMs from watsonx.ai.
credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
    # "api_key": "your api key here"
    # uncomment above when running locally
}

project_id = "skills-network"
model = Model(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)
#Build a model called `flan_ul2_llm` from watsonx.ai.
flan_ul2_llm = WatsonxLLM(model=model)


#This completes the LLM part of the Retrieval task.
#Integrating LangChain
#In the following steps, you create a simple Q&A application over the document source using LangChain's RetrievalQA.
#Then, you ask the query "what is mobile policy?"

qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "what is mobile policy?"
qa.invoke(query)

#From the response, it seems fine. The model's response is the relevant information about the mobile policy from the document.
#Now, try to ask a more high-level question.
qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "Can you summarize the document for me?"
qa.invoke(query)

#At this time, the model seems to not have the ability to summarize the document. This is because of the limitation of the `FLAN_UL2` model.
#So, try to use another model, `LLAMA_3_70B_INSTRUCT`. You should do the model construction again.
model_id = 'meta-llama/llama-3-3-70b-instruct'

parameters = {
    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5 # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = Model(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

llama_3_llm = WatsonxLLM(model=model)
#Try the same query again on this model.
qa = RetrievalQA.from_chain_type(llm=llama_3_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "Can you summarize the document for me?"
qa.invoke(query)

#Now, you've created a simple Q&A application for your own document. Congratulations!

#Dive deeper
#This section dives deeper into how you can improve this application. You might want to ask "How to add the prompt
# in retrieval using LangChain?"
#You use prompts to guide the responses from an LLM the way you want. For instance, if the LLM is uncertain about an
# answer, you instruct it to simply state, "I do not know," instead of attempting to generate a speculative response.
qa = RetrievalQA.from_chain_type(llm=flan_ul2_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "Can I eat in company vehicles?"
qa.invoke(query)

#Using prompt template
#In the following code, you create a prompt template using `PromptTemplate`.
prompt_template = """Use the information from the document to answer the question at the end. If you don't know the answer, just say that you don't know, definately do not try to make up an answer.

{context}

Question: {question}
"""

PROMPT = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

chain_type_kwargs = {"prompt": PROMPT}
#You can ask the same question that does not have an answer in the document again.
qa = RetrievalQA.from_chain_type(llm=llama_3_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 chain_type_kwargs=chain_type_kwargs,
                                 return_source_documents=False)

query = "Can I eat in company vehicles?"
qa.invoke(query)

#From the answer, you can see that the model responds with "don't know".


#Make the conversation have memory
#Take a look at a situation in which an LLM does not have memory.
query = "What I cannot do in it?"
qa.invoke(query)

#To make the LLM have memory, you introduce the ConversationBufferMemory function from LangChain.
memory = ConversationBufferMemory(memory_key = "chat_history", return_message = True)
#Create a `ConversationalRetrievalChain` to retrieve information and talk with the LLM.
qa = ConversationalRetrievalChain.from_llm(llm=llama_3_llm,
                                           chain_type="stuff",
                                           retriever=docsearch.as_retriever(),
                                           memory = memory,
                                           get_chat_history=lambda h : h,
                                           return_source_documents=False)
#Create a `history` list to store the chat history.
history = []

query = "What is mobile policy?"
result = qa.invoke({"question":query}, {"chat_history": history})
print(result["answer"])


#Append the previous query and answer to the history.
history.append((query, result["answer"]))

query = "List points in it?"
result = qa({"question": query}, {"chat_history": history})
print(result["answer"])

#Append the previous query and answer to the chat history again.
history.append((query, result["answer"]))

query = "What is the aim of it?"
result = qa({"question": query}, {"chat_history": history})
print(result["answer"])

#Wrap up and make it an agent
#The following code defines a function to make an agent, which can retrieve information from the document
# and has the conversation memory.
def qa():
    memory = ConversationBufferMemory(memory_key = "chat_history", return_message = True)
    qa = ConversationalRetrievalChain.from_llm(llm=llama_3_llm,
                                               chain_type="stuff",
                                               retriever=docsearch.as_retriever(),
                                               memory = memory,
                                               get_chat_history=lambda h : h,
                                               return_source_documents=False)
    history = []
    while True:
        query = input("Question: ")

        if query.lower() in ["quit","exit","bye"]:
            print("Answer: Goodbye!")
            break

        result = qa({"question": query}, {"chat_history": history})

        history.append((query, result["answer"]))

        print("Answer: ", result["answer"])


#To **stop** the agent, you can type in 'quit', 'exit', 'bye'. Otherwise you cannot run other cells.
qa()


#Work on your own document
filename = 'stateOfUnion.txt'
url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/XVnuuEg94sAE4S_xAsGxBA.txt'

wget.download(url, out=filename)
print('file downloaded')

#Return the source from the document
#All you must do is change the return_source_documents to True when you create the chain. And when you print, print the ['source_documents'][0]
qa = RetrievalQA.from_chain_type(llm=llama_3_llm, chain_type="stuff", retriever=docsearch.as_retriever(), return_source_documents=True)
query = "Can I smoke in company vehicles?"
results = qa.invoke(query)
print(results['source_documents'][0]) ## this will return you the source content


#Use another LLM model
#IBM watsonx.ai also has many other LLM models that you can use; for example, mistralai/mixtral-8x7b-instruct-v01,
# an open-source model from Mistral AI. Can you change the model to see the difference of the response?
#To use a different LLM, go to the cell where the model_id is specified and replace the current model_id with the
# following code. Expect different results and performance when using different LLMs:

model_id = 'mistralai/mixtral-8x7b-instruct-v01'