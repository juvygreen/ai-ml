# !pip install --force-reinstall --no-cache-dir tenacity --user
# !pip install "ibm-watsonx-ai==1.0.4" --user
# !pip install "ibm-watson-machine-learning==1.0.357" --user
# !pip install "langchain-ibm==0.1.7" --user
# !pip install "langchain-community==0.2.1" --user
# !pip install "langchain-experimental==0.0.59" --user
# !pip install "langchainhub==0.1.17" --user
# !pip install "langchain==0.2.1" --user
# !pip install "pypdf==4.2.0" --user
# !pip install "chromadb == 0.4.24" –user

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM

#Model
#The following will construct a ibm/granite-3-2-8b-instruct watsonx.ai inference model object:
model_id = 'ibm/granite-3-2-8b-instruct'

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = ModelInference(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

#Let's use a simple example to let the model generate some text:
msg = model.generate("In today's sales meeting, we ")
print(msg['results'][0]['generated_text'])
# ate a pizza lunch and discussed our progress on various projects. We also brainstormed new ideas for future projects and shared feedback on our current work. The meeting was productive and engaging, with everyone contributing their thoughts and ideas.
#
# In the afternoon, I worked on updating our project timeline and ensuring that all tasks were assigned and accounted for. I also reached out to a few team members to clarify some details and provide updates on my progress.
#
# In the evening, I attended a networking event hosted by a local business association. I had the opportunity to meet new people and discuss potential collaborations with other businesses. I exchanged contact information with several individuals and followed up with a few of them after the event.
#
# Overall, it was a busy but rewarding day. I feel energized and motivated to continue working on our projects and expanding our network.

#Chat model
#Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from
# the AI, users, and instructions such as system messages.

granite_llm = WatsonxLLM(model = model)
#The following provides an example of an interaction with a `WatsonLLM()`-wrapped model:
print(granite_llm.invoke("Who is man's best friend?"))
# A. Dog
# B. Cat
# C. Horse
# D. Parrot
# man's best friend is a dog. Dogs have been domesticated for thousands of years and are known for their loyalty, companionship, and ability to assist humans in various tasks. The term "man's best friend" was popularized by an 18th-century poet, but it has since become a widely accepted phrase to describe the strong bond between humans and dogs.
#
# So, the correct answer is A. Dog.

#Chat message
#The chat model takes a list of messages as input and returns a message. All messages have a role and a content property.
# There are a few different types of messages. The most commonly used are the following:
# •	SystemMessage: Used for priming AI behavior, usually passed in as the first in a sequence of input messages.
# •	HumanMessage: Represents a message from a person interacting with the chat model.
# •	AIMessage: Represents a message from the chat model. This can be either text or a request to invoke a tool.
# More messages types can be found at https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages.
# The following imports the most common message type classes from LangChain:
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
msg = granite_llm.invoke(
    [
        SystemMessage(content="You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence"),
        HumanMessage(content="I enjoy mystery novels, what should I read?")
    ]
)

print(msg)


#Notice that the model responded with an `AI` message.
#You can use these message types to pass an entire chat history along with the AI's responses to the model:
msg = granite_llm.invoke(
    [
        SystemMessage(content="You are a supportive AI bot that suggests fitness activities to a user in one short sentence"),
        HumanMessage(content="I like high-intensity workouts, what should I do?"),
        AIMessage(content="You should try a CrossFit class"),
        HumanMessage(content="How often should I attend?")
    ]
)

print(msg)

#You can also exclude the system message if you want:
msg = granite_llm.invoke(
    [
        HumanMessage(content="What month follows June?")
    ]
)

print(msg)


#Prompt templates
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("Tell me one {adjective} joke about {topic}")
input_ = {"adjective": "funny", "topic": "cats"}  # create a dictionary to store the corresponding input to placeholders in prompt template

prompt.invoke(input_)

from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

input_ = {"topic": "cats"}

prompt.invoke(input_)


#Messages place holder
from langchain_core.prompts import MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

input_ = {"msgs": [HumanMessage(content="What is the day after Tuesday?")]}

prompt.invoke(input_)

chain = prompt | granite_llm
response = chain.invoke(input = input_)
print(response)


# Example selectors
# If you have a large number of examples, you may need to select which ones to include in the prompt.
# The Example Selector is the class responsible for doing so.
# Example selector types could based on:
# •	Similarity: Uses semantic similarity between inputs and examples to decide which examples to choose.
# •	MMR: Uses Max Marginal Relevance between inputs and examples to decide which examples to choose.
# •	Length: Selects examples based on how many can fit within a certain length
# •	Ngram: Uses ngram overlap between inputs and examples to decide which examples to choose.
from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    max_length=25,  # The maximum length that the formatted examples should be.
)
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

#An example with small input, so it selects all examples.
print(dynamic_prompt.format(adjective="big"))

#An example with long input, so it selects only one example.
long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print(dynamic_prompt.format(adjective=long_string))


#Output parsers
#JSON parser
#This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
output_parser = JsonOutputParser(pydantic_object=Joke)

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": format_instructions},
)

chain = prompt | granite_llm | output_parser

chain.invoke({"query": joke_query})


#Comma separated list parser
#This output parser can be used when you want to return a list of comma-separated items.
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="Answer the user query. {format_instructions}\nList five {subject}.",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions},
)

chain = prompt | granite_llm | output_parser

chain.invoke({"subject": "ice cream flavors"})


#Documents
#Document object
#A Document object in LangChain contains information about some data. It has two attributes:
from langchain_core.documents import Document

Document(page_content="""Python is an interpreted high-level general-purpose programming language. 
                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.""",
         metadata={
             'my_document_id' : 234234,
             'my_document_source' : "About Python",
             'my_document_create_time' : 1680013019
         })


Document(page_content="""Python is an interpreted high-level general-purpose programming language. 
                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.""")


#Document loaders
#PDF loader
#By using the PDF loader, you can load a PDF file as a Document object.
#In this case, you are loading a paper about LangChain. You can access and read the paper at https://doi.org/10.48550/arXiv.2403.05568.
from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf")

document = loader.load()

document[2]  # take a look at the page 2


print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens


#URL and website loader
#You can also load content from a URL or website into a Document object:
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://python.langchain.com/v0.2/docs/introduction/")

web_data = loader.load()

print(web_data[0].page_content[:1000])


#Text splitters
# Once you've loaded documents, you'll often want to transform them to better suit your application.
# At a high level, text splitters work as follows:
# 1.	Split the text up into small, semantically meaningful chunks (often sentences).
# 2.	Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).
# 3.	Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).
# Here is a list of types of text splitters LangChain support.
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator="\n")  # define chunk_size which is length of characters, and also separator.
chunks = text_splitter.split_documents(document)
print(len(chunks))
#148

#It splits the document into 148 chunks. Let's look at the content of a chunk:
chunks[5].page_content   # take a look at any chunk's page content


#Embedding models
#Embedding models are specifically designed to interface with text embeddings.
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

embed_params = {
    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
    EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
}

from langchain_ibm import WatsonxEmbeddings

watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="skills-network",
    params=embed_params,
)

#The following embeds content in each of the chunks. You can then output the first 5 numbers in the vector
# representation of the content of the first chunk:
texts = [text.page_content for text in chunks]

embedding_result = watsonx_embedding.embed_documents(texts)
embedding_result[0][:5]




#Vector stores
#One of the most common ways to store and search over unstructured data is to embed it and store the resulting
# embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors
# that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.
from langchain.vectorstores import Chroma
#You have the embedding model perform the embedding process and store the resulting vectors in the Chroma vector database.
docsearch = Chroma.from_documents(chunks, watsonx_embedding)

#Then, you could use a similarity search strategy to retrieve the information that is related to the query you set.
#The model will return a list of similar/relevant document chunks. Here, you can print the contents of the most similar chunk:

query = "Langchain"
docs = docsearch.similarity_search(query)
print(docs[0].page_content)


#Retrievers
#A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.
# A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used
# as the backbone of a retriever, but there are other types of retrievers as well.
#Vector store-backed retriever
#A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper
# around the vector store class to make it conform to the retriever interface. It uses the search methods implemented
# by a vector store, like similarity search and MMR (Maximum marginal relevance), to query the texts in the vector store.
retriever = docsearch.as_retriever()

docs = retriever.invoke("Langchain")

docs[0]


#Note that the results are identical to the ones obtained using the similarity search strategy.

#Parent document retriever
#When splitting documents for retrieval, there are often conflicting desires:
#The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data.
# During retrieval, it first fetches the small chunks but then looks up the parent IDs for them and returns those
# larger documents.
from langchain.retrievers import ParentDocumentRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore

# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)
parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\n')
child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\n')

vectorstore = Chroma(
    collection_name="split_parents", embedding_function=watsonx_embedding
)

# The storage layer for the parent documents
store = InMemoryStore()

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

retriever.add_documents(document)
#These are a number of large chunks.
len(list(store.yield_keys()))
16

#Let's make sure the underlying vector store still retrieves the small chunks.
sub_docs = vectorstore.similarity_search("Langchain")

print(sub_docs[0].page_content)

#And then retrieve the relevant large chunk.

retrieved_docs = retriever.invoke("Langchain")

print(retrieved_docs[0].page_content)


#RetrievalQA
#Now that you understand how to retrieve information from a document, you might be interested in exploring some more
# exciting applications. For instance, you could have the Language Model (LLM) read the paper and summarize it for you,
# or create a QA bot that can answer your questions based on the paper.
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(llm=granite_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "what is this paper discussing?"
qa.invoke(query)


#Memory
#Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer
# to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to
# access some window of past messages directly.
#Chat message history
#One of the core utility classes underpinning most (if not all) memory modules is the ChatMessageHistory class.
# This is a super lightweight wrapper that provides convenience methods for saving HumanMessages, AIMessages, and then fetching them all.
from langchain.memory import ChatMessageHistory

chat = granite_llm
history = ChatMessageHistory()
history.add_ai_message("hi!")
history.add_user_message("what is the capital of France?")

history.messages

ai_response = chat.invoke(history.messages)
ai_response

history.add_ai_message(ai_response)
history.messages


#### Conversation buffer
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

conversation = ConversationChain(
    llm=granite_llm,
    verbose=True,
    memory=ConversationBufferMemory()
)

conversation.invoke(input="Hello, I am a little cat. Who are you?")


conversation.invoke(input="What can you do?")


conversation.invoke(input="Who am I?.")



#Chains
#Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step.
#It combines different LLM calls and actions automatically.
#Ex: Summary #1, Summary #2, Summary #3 > Final Summary
#Simple LLMChain
from langchain.chains import LLMChain
template = """Your job is to come up with a classic dish from the area that the users suggests.
                {location}

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['location'])

# chain 1
location_chain = LLMChain(llm=granite_llm, prompt=prompt_template, output_key='meal')

location_chain.invoke(input={'location':'China'})


#Simple sequential chain
#Sequential chains allow the output of one LLM to be used as the input for another. This approach is beneficial for
# dividing tasks and maintaining the focus of your LLM.

from langchain.chains import SequentialChain
template = """Given a meal {meal}, give a short and simple recipe on how to make that dish at home.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['meal'])

# chain 2
dish_chain = LLMChain(llm=granite_llm, prompt=prompt_template, output_key='recipe')

template = """Given the recipe {recipe}, estimate how much time I need to cook it.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['recipe'])

# chain 3
recipe_chain = LLMChain(llm=granite_llm, prompt=prompt_template, output_key='time')

# overall chain
overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],
                                      input_variables=['location'],
                                      output_variables=['meal', 'recipe', 'time'],
                                      verbose= True)

from pprint import pprint

pprint(overall_chain.invoke(input={'location':'China'}))


#Summarization chain
#Here is an example of using load_summarize_chain to summarize content.
#Let's use the web_data that you loaded from LangChain before as the content that needs to be summarized.
from langchain.chains.summarize import load_summarize_chain
chain = load_summarize_chain(llm=granite_llm, chain_type="stuff", verbose=False)
response = chain.invoke(web_data)
print(response['output_text'])


#Agents
#Tools
#Tools are interfaces that an agent, a chain, or a chat model / LLM can use to interact with the world.
#You can find a list of tools that LangChain supports at https://python.langchain.com/v0.1/docs/integrations/tools/.
from langchain.agents import Tool
from langchain_experimental.utilities import PythonREPL
python_repl = PythonREPL()
python_repl.run("a = 3; b = 1; print(a+b)")


#Toolkits
#Toolkits are collections of tools that are designed to be used together for specific tasks.
from langchain_experimental.tools import PythonREPLTool
tools = [PythonREPLTool()]

#A list of toolkits that Langchain supports is available at https://python.langchain.com/v0.1/docs/integrations/toolkits/.

#Agents
#By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating
# agents. Agents are systems that use an LLM as a reasoning engineer to determine which actions to take and what the
# inputs to those actions should be. The results of those actions can then be fed back into the agent.
# The agent then makes a determination whether more actions are needed, or whether it is okay to finish.
from langchain.agents import create_react_agent
from langchain import hub
from langchain.agents import AgentExecutor

instructions = """You are an agent designed to write and execute python code to answer questions.
You have access to a python REPL, which you can use to execute python code.
If you get an error, debug your code and try again.
Only use the output of your code to answer the question. 
You might know the answer without running any code, but you should still run the code to get the answer.
If it does not seem like you can write code to answer the question, just return "I don't know" as the answer.
"""

# here you will use the prompt directly from the langchain hub
base_prompt = hub.pull("langchain-ai/react-agent-template")
prompt = base_prompt.partial(instructions=instructions)

#set `verbose=True` to see how the LLM thinks and acts at every step.
agent = create_react_agent(granite_llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)  # tools were defined in the toolkit part above
agent_executor.invoke(input = {"input": "What is the 3rd fibonacci number?"})



#Try with another LLM
#Watsonx.ai provides access to several foundational models. In this lab, used ibm/granite-3-2-8b-instruct has been used.
# Try using another foundational model, such as meta-llama/llama-4-maverick-17b-128e-instruct-fp8.
model_id = 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8'

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = ModelInference(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

llama_llm = WatsonxLLM(model=model)

#Split the document with another separator
#use another separator to split the document and see how types of chunks are created? For example, use ["\n\n", "\n", ". ", " ", ""] as separators.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20, separators=["\n\n", "\n", ". ", " ", ""])
chunks = text_splitter.split_documents(document)
print(len(chunks))

print(chunks[5].page_content)


#Create an agent to talk with CSV data
#Imagine you have a CSV file that you would like an LLM to read and analyze for you.
# This way, you only need to ask the LLM, and it can return the answer to you.
from langchain.agents.agent_types import AgentType
from langchain_experimental.agents.agent_toolkits import create_csv_agent
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
import pandas as pd

df = pd.read_csv(
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv"
)

agent = create_pandas_dataframe_agent(
    llama_llm,
    df,
    verbose=True,
    return_intermediate_steps=True
)

response = agent.invoke("How many rows in the dataframe?",handle_parsing_errors=True)

print(response['output'])