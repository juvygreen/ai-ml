# Install required libraries
# For this lab, use the following libraries, which are not preinstalled in the Skills Network Labs environment. You can install libraries by running the code in the below cell.
#
# !pip install --user -qq datasets==2.20.0 trl==0.9.6 transformers==4.42.3 peft==0.11.1 tqdm==4.66.4 numpy==1.26.4 pandas==2.2.2 matplotlib==3.9.1 seaborn==0.13.2 scikit-learn==1.5.1 sacrebleu==2.4.2 evaluate==0.4.2
# !pip install --upgrade evaluate

import warnings
warnings.filterwarnings('ignore')
warnings.simplefilter('ignore')

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from datasets import load_dataset
import torch
from torch.utils.data import Dataset
from tqdm import tqdm
import evaluate
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM

from peft import get_peft_model, LoraConfig, TaskType

import pickle
import json
import matplotlib.pyplot as plt

from urllib.request import urlopen
import io

#Define the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#Dataset description
#Use the below sentences to download the CodeAlpaca 20k dataset, a programming code dataset.
# This code is available here. The CodeAlpaca dataset contains the following elements:
#•	instruction: str, describes the task the model should perform. Each of the 20K instructions is unique.
#•	input: str, optional context or input for the task. For example, when the instruction is
# "Amend the following SQL query to select distinct elements", the input is the SQL query.
# Around 40% of the examples have an input.
#•	output: str, the answer to the instruction as generated by text-davinci-003.
#The following code block downloads the CodeAlpaca-20k dataset as a json file:

!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WzOT_CwDALWedTtXjwH7bA/CodeAlpaca-20k.json

#Next we will load the dataset as follows:
dataset = load_dataset("json", data_files="CodeAlpaca-20k.json", split="train")
dataset

#Let's look at the example in the dataset:
dataset[1000]
#{'output': 's = "Hello world" \ns = s[::-1] \nprint(s)',
# 'instruction': 'Reverse the string given in the input',
# 'input': 'Hello world'}

#To keep things simple let's just focus on the examples that do not have any `input`:
dataset = dataset.filter(lambda example: example["input"] == '')
#Filter: 100%
# 20022/20022 [00:00<00:00, 145197.53 examples/s]

#The original CodeAlpaca dataset may not have been shuffled. The following line indicates how to shuffle a
# `datasets.arrow_dataset.Dataset()` object with a random seed:
dataset = dataset.shuffle(seed=42)
dataset
# Dataset({
#     features: ['output', 'instruction', 'input'],
#     num_rows: 9764
# })
#
# The CodeAlpaca 20k dataset has a training and test set. You can split the original training data into a train
# and test set by assigning 80% of the data to the training set and 20% to the testing set.
dataset_split = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = dataset_split['train']
test_dataset = dataset_split['test']
dataset_split
# DatasetDict({
#     train: Dataset({
#         features: ['output', 'instruction', 'input'],
#         num_rows: 7811
#     })
#     test: Dataset({
#         features: ['output', 'instruction', 'input'],
#         num_rows: 1953
#     })
# })

# Select a small set of data for the resource limitation
# This dataset will be only used for evaluation parts, not for the training
tiny_test_dataset=test_dataset.select(range(10))
tiny_train_dataset=train_dataset.select(range(10))

#Model and tokenizer
# Base model
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m").to(device)

#This model comes with its own tokenizer which you will be loading here:
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m", padding_side='left')

tokenizer.eos_token
#'</s>'

#Preprocessing the data
### Instruction:
#Translate the following sentence to Spanish: "Hello, how are you?"

### Response:
#"Hola, ¿cómo estás?</s>"
def formatting_prompts_func(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Instruction:\n{mydataset['instruction'][i]}"
            f"\n\n### Response:\n{mydataset['output'][i]}</s>"
        )
        output_texts.append(text)
    return output_texts

def formatting_prompts_func_no_response(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Instruction:\n{mydataset['instruction'][i]}"
            f"\n\n### Response:\n"
        )
        output_texts.append(text)
    return output_texts

expected_outputs = []
instructions_with_responses = formatting_prompts_func(test_dataset)
instructions = formatting_prompts_func_no_response(test_dataset)
for i in tqdm(range(len(instructions_with_responses))):
    tokenized_instruction_with_response = tokenizer(instructions_with_responses[i], return_tensors="pt", max_length=1024, truncation=True, padding=False)
    tokenized_instruction = tokenizer(instructions[i], return_tensors="pt")
    expected_output = tokenizer.decode(tokenized_instruction_with_response['input_ids'][0][len(tokenized_instruction['input_ids'][0])-1:], skip_special_tokens=True)
    expected_outputs.append(expected_output)
#100%|██████████| 1953/1953 [00:01<00:00, 1636.08it/s]

#Let's look at the example to view what `instructions` include, `instructions_with_responses`, and `expected_outputs`:
print('############## instructions ##############\n' + instructions[0])
print('############## instructions_with_responses ##############\n' + instructions_with_responses[0])
print('\n############## expected_outputs ##############' + expected_outputs[0])

class ListDataset(Dataset):
    def __init__(self, original_list):
        self.original_list = original_list

    def __len__(self):
        return len(self.original_list)

    def __getitem__(self, i):
        return self.original_list[i]

instructions_torch = ListDataset(instructions)
instructions_torch[0]
#'### Instruction:\nName the most important benefit of using a database system.\n\n### Response:\n'

#Test the base model
gen_pipeline = pipeline("text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=device,
                        batch_size=2,
                        max_length=50,
                        truncation=True,
                        padding=False,
                        return_full_text=False)

tokenizer.padding_side = 'left'

with torch.no_grad():
    # Due to resource limitation, only apply the function on 3 records using "instructions_torch[:10]"
    pipeline_iterator= gen_pipeline(instructions_torch[:3],
                                    max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice
                                    num_beams=5,
                                    early_stopping=True,)

generated_outputs_base = []
for text in pipeline_iterator:
    generated_outputs_base.append(text[0]["generated_text"])

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VvQRrSqS1P0_GobqtL-SKA/instruction-tuning-generated-outputs-base.pkl')
generated_outputs_base = pickle.load(io.BytesIO(urlopened.read()))
for i in range(3):
    print('@@@@@@@@@@@@@@@@@@@@')
    print('@@@@@ Instruction '+ str(i+1) +': ')
    print(instructions[i])
    print('\n\n')
    print('@@@@@ Expected response '+ str(i+1) +': ')
    print(expected_outputs[i])
    print('\n\n')
    print('@@@@@ Generated response '+ str(i+1) +': ')
    print(generated_outputs_base[i])
    print('\n\n')
    print('@@@@@@@@@@@@@@@@@@@@')


# BLEU score
# Let's set up a metric that compares the generated responses and the expected responses in the test environment.
# In this lab, let's use the BLEU score, a metric originally intended to check the quality of translations made by
# translation models. You can calculate the BLEU scores for individual generated segments by comparing them with a
# set of expected outputs and average the scores for the individual segments. Depending on the implementation,
# BLEU scores range from 0 to 1 or from 0 to 100 (as in the implementation used herein), with higher scores
# indicating a better match between the model generated output and the expected output.


sacrebleu = evaluate.load("sacrebleu")
results_base = sacrebleu.compute(predictions=generated_outputs_base,
                                 references=expected_outputs)

print(list(results_base.keys()))
print(round(results_base["score"], 1))

# Perform instruction fine-tuning with LoRA
lora_config = LoraConfig(
    r=16,  # Low-rank dimension
    lora_alpha=32,  # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Modules to apply LoRA
    lora_dropout=0.1,  # Dropout rate
    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model
)

model = get_peft_model(model, lora_config)

response_template = "### Response:\n"
collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)
#To perform the training, first configure our `SFTTrainer`, and create the `SFTTrainer` object by passing to the `collator`:
training_args = SFTConfig(
    output_dir="/tmp",
    num_train_epochs=10,
    save_strategy="epoch",
    fp16=True,
    per_device_train_batch_size=2,  # Reduce batch size
    per_device_eval_batch_size=2,  # Reduce batch size
    max_seq_length=1024,
    do_eval=True
)

trainer = SFTTrainer(
    model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    formatting_func=formatting_prompts_func,
    args=training_args,
    packing=False,
    data_collator=collator,
)

#The below comments, runs the trainer, because this would take a long time on the CPU. Therefore, let's not run the trainer here.
#trainer.train()
#log_history_lora = trainer.state.log_history

#Instead of extracting the state history above, let's load the state history of a model that was instruction fine-tuned to the above specifications on a GPU.
urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/49I70jQD0-RNRg2v-eOoxg/instruction-tuning-log-history-lora.json')
log_history_lora = json.load(io.BytesIO(urlopened.read()))
train_loss = [log["loss"] for log in log_history_lora if "loss" in log]

# Plot the training loss
plt.figure(figsize=(10, 5))
plt.plot(train_loss, label='Training Loss')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.legend()
plt.show()
#If you want to fine-tune the model, the fine-tuned model could be saved using the below commented out line:
#trainer.save_model("./instruction_tuning_final_model_lora")

#Let's redefine the text generation pipeline because the model has been changed to the LoRA model.
# Ignore the warning for the `PeftModelForCausalLM` not being supported for `text-generation`.
# However, if the PEFT model is supported, the warning is erroneous.
gen_pipeline = pipeline("text-generation",
                        model=model,
                        tokenizer=tokenizer,
                        device=device,
                        batch_size=2,
                        max_length=50,
                        truncation=True,
                        padding=False,
                        return_full_text=False)

with torch.no_grad():
    # Due to resource limitation, only apply the function on 3 records using "instructions_torch[:10]"
    pipeline_iterator= gen_pipeline(instructions_torch[:3],
                                max_length=50, # this is set to 50 due to resource constraint, using a GPU, you can increase it to the length of your choice
                                num_beams=5,
                                early_stopping=True,)
generated_outputs_lora = []
for text in pipeline_iterator:
    generated_outputs_lora.append(text[0]["generated_text"])

generated_outputs_lora[0]

urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/o7uYxe15xvX4CN-6Lr10iA/instruction-tuning-generated-outputs-lora.pkl')
generated_outputs_lora = pickle.load(io.BytesIO(urlopened.read()))
for i in range(3):
    print('@@@@@@@@@@@@@@@@@@@@')
    print('@@@@@ Instruction '+ str(i+1) +': ')
    print(instructions[i])
    print('\n\n')
    print('@@@@@ Expected response '+ str(i+1) +': ')
    print(expected_outputs[i])
    print('\n\n')
    print('@@@@@ Generated response '+ str(i+1) +': ')
    print(generated_outputs_lora[i])
    print('\n\n')
    print('@@@@@@@@@@@@@@@@@@@@')


sacrebleu = evaluate.load("sacrebleu")
results_lora = sacrebleu.compute(predictions=generated_outputs_lora,
                                 references=expected_outputs)
print(list(results_lora.keys()))
print(round(results_lora["score"], 1))


#Try with another response template (Question-Answering)
#Create a formatting_prompts_response_template function to format the train_dataset in the Response Template.
#Template: ### Question: {question}\n ### Answer: {answer}
def formatting_prompts_response_template(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Question:\n{mydataset['instruction'][i]}"
            f"\n\n### Answer:\n{mydataset['output'][i]}</s>"
        )
        output_texts.append(text)
    return output_texts


#Template: ### Question: {question}\n ### Answer:
def formatting_prompts_response_template_no_response(mydataset):
    output_texts = []
    for i in range(len(mydataset['instruction'])):
        text = (
            f"### Question:\n{mydataset['instruction'][i]}"
            f"\n\n### Answer:\n"
        )
        output_texts.append(text)
    return output_texts


#Try with another LLM (EleutherAI/gpt-neo-125m)
#The EleutherAI/gpt-neo-125m is a smaller variant of the GPT-Neo family of models developed by EleutherAI.
# With 125 million parameters, it is designed to be computationally efficient while still providing robust
# performance for various natural language processing tasks.
#Download and load the EleutherAI/gpt-neo-125m model
model_name = "EleutherAI/gpt-neo-125m"

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Initialize LoRA Configuration:
# •	r: 8 (Low-rank dimension)
# •	lora_alpha: 16 (Scaling factor)
# •	target_modules: ["q_proj", "v_proj"] (Modules to apply LoRA)
# •	lora_dropout: 0.1 (Dropout rate)
# •	task_type: TaskType.CAUSAL_LM (Task type should be causal language model)
lora_config = LoraConfig(
    r=8,  # Low-rank dimension
    lora_alpha=16,  # Scaling factor
    target_modules=["q_proj", "v_proj"],  # Modules to apply LoRA
    lora_dropout=0.1,  # Dropout rate
    task_type=TaskType.CAUSAL_LM  # Task type should be causal language model
)
#Apply LoRA Configuration to the model.
model = get_peft_model(model, lora_config)
