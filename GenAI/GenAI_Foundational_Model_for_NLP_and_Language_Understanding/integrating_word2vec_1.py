# !pip install -q numpy pandas matplotlib seaborn scikit-learn
# After installing the libraries above please RESTART THE KERNEL and then run cells below
# %%capture
# !pip install gensim #4.2.0
# !pip install portalocker>=2.0.0
# !pip install torch==2.2.2
# !pip install torchdata==0.7.1
# !pip install torchtext==0.17.2


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE

from IPython.core.display import display, SVG

from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import Dataset

import logging
from gensim.models import Word2Vec
from collections import defaultdict
import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.vocab import GloVe, vocab
from torchdata.datapipes.iter import IterableWrapper, Mapper
from torchtext.datasets import AG_NEWS
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
from torchtext.data.utils import get_tokenizer
from torch.utils.data import DataLoader
from tqdm import tqdm

%matplotlib
inline


# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass


import warnings

warnings.warn = warn
warnings.filterwarnings('ignore')


#Define a function to plot word embeddings in a 2 dspace.
def plot_embeddings(word_embeddings, vocab=vocab):
    tsne = TSNE(n_components=2, random_state=0)
    word_embeddings_2d = tsne.fit_transform(word_embeddings)

    # Plotting the results with labels from vocab
    plt.figure(figsize=(15, 15))
    for i, word in enumerate(vocab.get_itos()):  # assuming vocab.itos gives the list of words in your vocab
        plt.scatter(word_embeddings_2d[i, 0], word_embeddings_2d[i, 1])
        plt.annotate(word, (word_embeddings_2d[i, 0], word_embeddings_2d[i, 1]))

    plt.xlabel("t-SNE component 1")
    plt.ylabel("t-SNE component 2")
    plt.title("Word Embeddings visualized with t-SNE")
    plt.show()


# This function returns the most similar words to a target word by calculating word vectors' cosine distance
def find_similar_words(word, word_embeddings, top_k=5):
    if word not in word_embeddings:
        print("Word not found in embeddings.")
        return []

    # Get the embedding for the given word
    target_embedding = word_embeddings[word]

    # Calculate cosine similarities between the target word and all other words
    similarities = {}
    for w, embedding in word_embeddings.items():
        if w != word:
            similarity = torch.dot(target_embedding, embedding) / (
                    torch.norm(target_embedding) * torch.norm(embedding)
            )
            similarities[w] = similarity.item()

    # Sort the similarities in descending order
    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)

    # Return the top k similar words
    most_similar_words = [w for w, _ in sorted_similarities[:top_k]]
    return most_similar_words


# Define a function that trains word2vec model on toy data.
def train_model(model, dataloader, criterion, optimizer, num_epochs=1000):
    """
    Train the model for the specified number of epochs.

    Args:
        model: The PyTorch model to be trained.
        dataloader: DataLoader providing data for training.
        criterion: Loss function.
        optimizer: Optimizer for updating model's weights.
        num_epochs: Number of epochs to train the model for.

    Returns:
        model: The trained model.
        epoch_losses: List of average losses for each epoch.
    """

    # List to store running loss for each epoch
    epoch_losses = []

    for epoch in tqdm(range(num_epochs)):
        # Storing running loss values for the current epoch
        running_loss = 0.0

        # Using tqdm for a progress bar
        for idx, samples in enumerate(dataloader):

            optimizer.zero_grad()

            # Check for EmbeddingBag layer in the model
            if any(isinstance(module, nn.EmbeddingBag) for _, module in model.named_modules()):
                target, context, offsets = samples
                predicted = model(context, offsets)

            # Check for Embedding layer in the model
            elif any(isinstance(module, nn.Embedding) for _, module in model.named_modules()):
                target, context = samples
                predicted = model(context)

            loss = criterion(predicted, target)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
            optimizer.step()
            running_loss += loss.item()

        # Append average loss for the epoch
        epoch_losses.append(running_loss / len(dataloader))

    return model, epoch_losses

# Word2Vec is a family of methods that transforms words into number vectors, positioning similar words close
# together in a space defined by these numbers. This way, you can quantify and analyze word relationships
# mathematically.
# GloVe, on the other hand, is another popular algorithm for learning word embeddings.
# It stands for Global Vectors for Word Representation. Unlike word2vec, which is based on predicting
# context/target words, GloVe focuses on capturing the global word co-occurrence statistics from the entire corpus.
# It constructs a co-occurrence matrix that represents how often words appear together in the text.


# Create and train word2vec models
# start with a very simple implementation of word2vec to train it on a toy dataset:

toy_data = """I wish I was little bit taller
I wish I was a baller
She wore a small black dress to the party
The dog chased a big red ball in the park
He had a huge smile on his face when he won the race
The tiny kitten played with a fluffy toy mouse
The team celebrated their victory with a grand parade
She bought a small, delicate necklace for her sister
The mountain peak stood majestic and tall against the clear blue sky
The toddler took small, careful steps as she learned to walk
The house had a spacious backyard with a big swimming pool
He felt a sense of accomplishment after completing the challenging puzzle
The chef prepared a delicious, flavorful dish using fresh ingredients
The children played happily in the small, cozy room
The book had an enormous impact on readers around the world
The wind blew gently, rustling the leaves of the tall trees
She painted a beautiful, intricate design on the small canvas
The concert hall was filled with thousands of excited fans
The garden was adorned with colorful flowers of all sizes
I hope to achieve great success in my chosen career path
The skyscraper towered above the city, casting a long shadow
He gazed in awe at the breathtaking view from the mountaintop
The artist created a stunning masterpiece with bold brushstrokes
The baby took her first steps, a small milestone that brought joy to her parents
The team put in a tremendous amount of effort to win the championship
The sun set behind the horizon, painting the sky in vibrant colors
The professor gave a fascinating lecture on the history of ancient civilizations
The house was filled with laughter and the sound of children playing
She received a warm, enthusiastic welcome from the audience
The marathon runner had incredible endurance and determination
The child's eyes sparkled with excitement upon opening the gift
The ship sailed across the vast ocean, guided by the stars
The company achieved remarkable growth in a short period of time
The team worked together harmoniously to complete the project
The puppy wagged its tail, expressing its happiness and affection
She wore a stunning gown that made her feel like a princess
The building had a grand entrance with towering columns
The concert was a roaring success, with the crowd cheering and clapping
The baby took a tiny bite of the sweet, juicy fruit
The athlete broke a new record, achieving a significant milestone in her career
The sculpture was a masterpiece of intricate details and craftsmanship
The forest was filled with towering trees, creating a sense of serenity
The children built a small sandcastle on the beach, their imaginations running wild
The mountain range stretched as far as the eye could see, majestic and awe-inspiring
The artist's brush glided smoothly across the canvas, creating a beautiful painting
She received a small token of appreciation for her hard work and dedication
The orchestra played a magnificent symphony that moved the audience to tears
The flower bloomed in vibrant colors, attracting butterflies and bees
The team celebrated their victory with a big, extravagant party
The child's laughter echoed through the small room, filling it with joy
The sunflower stood tall, reaching for the sky with its bright yellow petals
The city skyline was dominated by tall buildings and skyscrapers
The cake was adorned with a beautiful, elaborate design for the special occasion
The storm brought heavy rain and strong winds, causing widespread damage
The small boat sailed peacefully on the calm, glassy lake
The artist used bold strokes of color to create a striking and vivid painting
The couple shared a passionate kiss under the starry night sky
The mountain climber reached the summit after a long and arduous journey
The child's eyes widened in amazement as the magician performed his tricks
The garden was filled with the sweet fragrance of blooming flowers
The basketball player made a big jump and scored a spectacular slam dunk
The cat pounced on a small mouse, displaying its hunting instincts
The mansion had a grand entrance with a sweeping staircase and chandeliers
The raindrops fell gently, creating a rhythmic patter on the roof
The baby took a big step forward, encouraged by her parents' applause
The actor delivered a powerful and emotional performance on stage
The butterfly fluttered its delicate wings, mesmerizing those who watched
The company launched a small-scale advertising campaign to test the market
The building was constructed with strong, sturdy materials to withstand earthquakes
The singer's voice was powerful and resonated throughout the concert hall
The child built a massive sandcastle with towers, moats, and bridges
The garden was teeming with a variety of small insects and buzzing bees
The athlete's muscles were well-developed and strong from years of training
The sun cast long shadows as it set behind the mountains
The couple exchanged heartfelt vows in a beautiful, intimate ceremony
The dog wagged its tail vigorously, a sign of excitement and happiness
The baby let out a tiny giggle, bringing joy to everyone around"""

# prepare data by tokenizing it and creating a vocabulary from data.
# Step 1: Get tokenizer
tokenizer = get_tokenizer('basic_english')  # This uses basic English tokenizer. You can choose another.

# Step 2: Tokenize sentences
def tokenize_data(sentences):
    for sentence in sentences:
        yield tokenizer(sentence)

tokenized_toy_data = tokenizer (toy_data)


vocab = build_vocab_from_iterator(tokenize_data(tokenized_toy_data), specials=['<unk>'])
vocab.set_default_index(vocab["<unk>"])
# check how a sentence looks like after tokenization and numericalization:
# Test
sample_sentence = "I wish I was a baller"
tokenized_sample = tokenizer(sample_sentence)
encoded_sample = [vocab[token] for token in tokenized_sample]
print("Encoded sample:", encoded_sample)
#Encoded sample: [20, 108, 20, 7, 2, 133]

# write a function to apply numericalization on all tokens:
text_pipeline = lambda tokens:[ vocab[token]  for token in tokens]


# Continuous Bag of Words (CBOW)
# For the Continuous Bag of Words (CBOW) model, use a "context" to predict a target word. The "context" is typically
# a set of surrounding words. For example, if your context window is of size 2, then you take two words before and
# two words after the target word as context.

# slide over the sequence and create training data:
CONTEXT_SIZE = 2


cobow_data = []

# modified code

for i in range(CONTEXT_SIZE, len(tokenized_toy_data ) - CONTEXT_SIZE):

    context = (

        [tokenized_toy_data [i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)]

        + [tokenized_toy_data [i + j + 1] for j in range(CONTEXT_SIZE)]

    )

    target = tokenized_toy_data [i]

    cobow_data.append((context, target))

#print a sample, showcasing both the context words ['wish', 'i', 'was', 'little'] and the target word 'i':
print(cobow_data[0])

# print the next sample, showcasing both the context ['i', 'wish', 'little', 'bit'] and the target words:'was'
print(cobow_data[1])


def collate_batch(batch):
    target_list, context_list, offsets = [], [], [0]
    for _context, _target in batch:

        target_list.append(vocab[_target])
        processed_context = torch.tensor(text_pipeline(_context), dtype=torch.int64)
        context_list.append(processed_context)
        offsets.append(processed_context.size(0))
    target_list = torch.tensor(target_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    context_list = torch.cat(context_list)
    return target_list.to(device), context_list.to(device), offsets.to(device)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
# Selecting the first 10 samples from the cobow_data list and processing them using the collate_batch function. T
# he outputs are the tokenized target words (target_list), the surrounding context words (context_list),
# and the respective offsets for each sample (offsets).
target_list, context_list, offsets=collate_batch(cobow_data[0:10])
print(f"target_list(Tokenized target words): {target_list} , context_list(Surrounding context words): {context_list} , offsets(Starting indexes of context words for each target): {offsets} ")
# target_list(Tokenized target words): tensor([ 20,   7, 272, 136, 376,  20, 108,  20,   7,   2]) , context_list(Surrounding context words): tensor([ 20, 108,   7, 272, 108,  20, 272, 136,  20,   7, 136, 376,   7, 272,
#         376,  20, 272, 136,  20, 108, 136, 376, 108,  20, 376,  20,  20,   7,
#          20, 108,   7,   2, 108,  20,   2, 133,  20,   7, 133,  14]) , offsets(Starting indexes of context words for each target): tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36])

# Create a dataLoader object:
BATCH_SIZE = 64  # batch size for training

dataloader_cbow = DataLoader(
    cobow_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)
print(dataloader_cbow)


# The CBOW model shown here starts with an EmbeddingBag layer, which takes a variable-length list of context word
# indices and produces an averaged embedding of size embed_dim. This embedding is then passed through a linear
# layer that reduces its dimension to embed_dim/2. After applying a ReLU activation, the output is processed by
# another linear layer, transforming it to match the vocabulary size, thus allowing the model to predict the
# probability of any word from the vocabulary as the target word. The overall flow moves from contextual words'
# indices to predicting the central word in the Continuous Bag of Words approach.
class CBOW(nn.Module):
    # Initialize the CBOW model
    def __init__(self, vocab_size, embed_dim, num_class):

        super(CBOW, self).__init__()
         # Define the embedding layer using nn.EmbeddingBag
        # It outputs the average of context words embeddings
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        # Define the first linear layer with input size embed_dim and output size embed_dim//2
        self.linear1 = nn.Linear(embed_dim, embed_dim//2)
        # Define the fully connected layer with input size embed_dim//2 and output size vocab_size
        self.fc = nn.Linear(embed_dim//2, vocab_size)


        self.init_weights()
    # Initialize the weights of the model's parameters
    def init_weights(self):
        # Initialize the weights of the embedding layer
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        # Initialize the weights of the fully connected layer
        self.fc.weight.data.uniform_(-initrange, initrange)
        # Initialize the biases of the fully connected layer to zeros
        self.fc.bias.data.zero_()


    def forward(self, text, offsets):
        # Pass the input text and offsets through the embedding layer
        out = self.embedding(text, offsets)
        # Apply the ReLU activation function to the output of the first linear layer
        out = torch.relu(self.linear1(out))
        # Pass the output of the ReLU activation through the fully connected layer
        return self.fc(out)

#Create an instance of the CBOW model:
vocab_size = len(vocab)
emsize = 24
model_cbow = CBOW(vocab_size, emsize, vocab_size).to(device)
#Define the loss function, optimizer, and scheduler for training:

LR = 5  # learning rate

# Define the CrossEntropyLoss criterion. It is commonly used for multi-class classification tasks.
# This criterion combines the softmax function and the negative log-likelihood loss.
criterion = torch.nn.CrossEntropyLoss()

# Define the optimizer using stochastic gradient descent (SGD).
# It optimizes the parameters of the model_cbow, which are obtained by model_cbow.parameters().
# The learning rate (lr) determines the step size for parameter updates during optimization.
optimizer = torch.optim.SGD(model_cbow.parameters(), lr=LR)

# Define a learning rate scheduler.
# The StepLR scheduler adjusts the learning rate during training.
# It multiplies the learning rate by gamma every step_size epochs (here, 1.0).
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
#train the model:
model_cbow, epoch_losses=train_model(model_cbow, dataloader_cbow, criterion, optimizer, num_epochs=400)
#plot the loss values over the course of training:
plt.plot(epoch_losses)
plt.xlabel("epochs")


# The model's weights are the actual word embeddings. You can load them into a numpy array:
# word_embeddings = model_cbow.embedding.weight.detach().cpu().numpy()
# check the embedded vector for a sample word. Notice the shape of this vector which is equal to the
# emsize = 24 that you defined earlier.
word = 'baller'
word_index = vocab.get_stoi()[word] # getting the index of the word in the vocab
print(word_embeddings[word_index])
# [-0.79776794 -0.45578814  0.549084    0.00563984  0.07812627 -0.00641155
#  -0.5671006   0.4119515   0.71738225  0.5358058  -0.23532939  0.23275268
#   0.27972054 -0.06707755 -0.7902719   0.02771408  0.26122466  0.01407974
#  -0.01083941  0.338167   -0.24912675 -0.05963361  0.06806811  0.14320555]
#
# Now you can check if embeddings are representing the similarities among words. To do this, for the sake of
# visualization,you need to do dimension reduction on word embeddings to map the embedding space to a 2-d space.
# You can do this using TSNE in the plot function in the helper functions section.
plot_embeddings(word_embeddings,vocab=vocab)


# Skip-gram model
# The Skip-gram model is one of the two main architectures used in word2vec, a popular technique for learning word
# embeddings. In the Skip-gram model, the goal is to predict the surrounding words (context) given a central
# word (target). The main idea behind this model is that words that appear in similar contexts tend
# to have similar meanings

# This code constructs a skip-gram dataset from a tokenized toy data, where for each word (target),
# it gathers the surrounding words within a specified window (context) defined by CONTEXT_SIZE.
# Define the window size for the context around the target word.
CONTEXT_SIZE = 2

# Initialize an empty list to store the (target, context) pairs.
skip_data = []

# Iterate over each word in the tokenized toy_data, while excluding the first
# and last few words determined by the CONTEXT_SIZE.
for i in range(CONTEXT_SIZE, len(tokenized_toy_data) - CONTEXT_SIZE):

    # For a word at position i, the context comprises of words from the preceding CONTEXT_SIZE
    # as well as from the succeeding CONTEXT_SIZE. The context words are collected in a list.
    context = (
        [tokenized_toy_data[i - j - 1] for j in range(CONTEXT_SIZE)]  # Preceding words
        + [tokenized_toy_data[i + j + 1] for j in range(CONTEXT_SIZE)]  # Succeeding words
    )

    # The word at the current position i is taken as the target.
    target = tokenized_toy_data[i]

    # Append the (target, context) pair to the skip_data list.
    skip_data.append((target, context))

# window the skipgram
skip_data_=[[(sample[0],word) for word in  sample[1]] for sample in skip_data]
# will have pairs of (target, context) words:
skip_data_flat= [item  for items in  skip_data_ for item in items]
skip_data_flat[8:28]
# [('little', 'was'),
#  ('little', 'i'),
#  ('little', 'bit'),
#  ('little', 'taller'),
#  ('bit', 'little'),
#  ('bit', 'was'),
#  ('bit', 'taller'),
#  ('bit', 'i'),
#  ('taller', 'bit'),
#  ('taller', 'little'),
#  ('taller', 'i'),
#  ('taller', 'wish'),
#  ('i', 'taller'),
#  ('i', 'bit'),
#  ('i', 'wish'),
#  ('i', 'i'),
#  ('wish', 'i'),
#  ('wish', 'taller'),
#  ('wish', 'i'),
#  ('wish', 'was')]
#
# Creating a collate function to numericalize (target, context) pairs:
def collate_fn(batch):
    target_list, context_list = [], []
    for _context, _target in batch:

        target_list.append(vocab[_target])
        context_list.append(vocab[_context])

    target_list = torch.tensor(target_list, dtype=torch.int64)
    context_list = torch.tensor(context_list, dtype=torch.int64)
    return target_list.to(device), context_list.to(device)
dataloader = DataLoader(skip_data_flat, batch_size=BATCH_SIZE, collate_fn=collate_fn)

# check a sample batch of target,context after collation:
next(iter(dataloader))
# (tensor([108,  20,   7, 272,  20, 108, 272, 136,   7,  20, 136, 376, 272,   7,
#          376,  20, 136, 272,  20, 108, 376, 136, 108,  20,  20, 376,  20,   7,
#          108,  20,   7,   2,  20, 108,   2, 133,   7,  20, 133,  14,   2,   7,
#           14, 109, 133,   2, 109,   2,  14, 133,   2,   8, 109,  14,   8, 138,
#            2, 109, 138, 198,   8,   2, 198,  10]),
#  tensor([ 20,  20,  20,  20,   7,   7,   7,   7, 272, 272, 272, 272, 136, 136,
#          136, 136, 376, 376, 376, 376,  20,  20,  20,  20, 108, 108, 108, 108,
#           20,  20,  20,  20,   7,   7,   7,   7,   2,   2,   2,   2, 133, 133,
#          133, 133,  14,  14,  14,  14, 109, 109, 109, 109,   2,   2,   2,   2,
#            8,   8,   8,   8, 138, 138, 138, 138]))

# define the Skip-gram network. The embeddings layer is defined using nn.Embedding, which creates word embeddings
# for the given vocabulary size and embedding dimension. The fc layer is a fully connected layer with input
# dimension embed_dim and output dimension vocab_size.
# In the forward method, the input text is passed through the embeddings layer to obtain the word embeddings.
# The output of the embeddings layer is then passed through the fc layer. The ReLU activation function is
# applied to the output of the fc layer. The final output is returned.

class SkipGram_Model(nn.Module):

    def __init__(self, vocab_size, embed_dim):
        super(SkipGram_Model, self).__init__()
        # Define the embeddings layer
        self.embeddings = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim
        )

        # Define the fully connected layer
        self.fc = nn.Linear(in_features=embed_dim, out_features=vocab_size)

    def forward(self, text):
        # Perform the forward pass
        # Pass the input text through the embeddings layer
        out = self.embeddings(text)

        # Pass the output of the embeddings layer through the fully connected layer
        # Apply the ReLU activation function
        out = torch.relu(out)
        out = self.fc(out)

        return out

# Creating an instance of the model:
emsize = 24
model_sg = SkipGram_Model(vocab_size, emsize).to(device)
#train the model on toy data:
LR = 5  # learning rate
#BATCH_SIZE = 64  # batch size for training

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model_sg.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
model_sg, epoch_losses=train_model(model_sg, dataloader, criterion, optimizer, num_epochs=400)
plt.plot(epoch_losses)

# also plot the word embedding by reducing the dimensions using t-SNE:
word_embeddings = model_sg.embeddings.weight.detach().cpu().numpy()
plot_embeddings(word_embeddings,vocab=vocab)