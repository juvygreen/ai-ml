# Note: If your environment doesn't support "!pip install", use "!mamba install"
# You may comment %%capture line below if you do not want to supress the messages that appear during package installation
#
# %%capture
#
# !pip install -q numpy pandas matplotlib seaborn scikit-learn
# # - Update a specific package
# !pip install pmdarima -U
# # - Update a package to specific version
# !pip install --upgrade pmdarima==2.0.2
#
# !pip install -Uqq portalocker>=2.0.0
# !pip install -qq torchtext
# !pip install -qq torchdata
# !pip install -Uqq plotly
# !pip install -qq dash
#
# %%capture
#
from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
from torchtext.data.utils import get_tokenizer

import torch
import torch.nn as nn

from torch.utils.data import DataLoader
from torchtext.datasets import AG_NEWS
from IPython.display import Markdown as md

from torchtext.vocab import build_vocab_from_iterator
from torchtext.datasets import AG_NEWS
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
from sklearn.manifold import TSNE
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split

from torchtext.data.utils import get_tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

#Defining helper functions
#Use this section to define any helper functions to help the notebook's code readability:
def plot(COST,ACC):
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()  # otherwise the right y-label is slightly clipped

    plt.show()


# Text classification
# Let's build a text classification model using PyTorch and torchtext to classify news articles into one of the four
# categories: World, Sports, Business, and Sci/Tech.
# Import bank dataset
# Load the AG_NEWS dataset for the train split and split it into input text and corresponding labels:
train_iter= iter(AG_NEWS(split="train"))
# The AG_NEWS dataset in torchtext does not support direct indexing like a list or tuple. It is not a random access
# dataset but rather an iterable dataset that needs to be used with an iterator. This approach is more effective
# for text data.
y,text= next((train_iter))
print(y,text)


# Find the label of the sample.
ag_news_label = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tec"}
ag_news_label[y]
# 'Business'

# Also, use the dataset to find all the classes.
num_class = len(set([label for (label, text) in train_iter ]))
num_class
# 4

# Create the tokens as explained in previous lab and also build the vocabulary as before, just using the AG dataset to obtain token indices
# Reinitialize train_iter
train_iter = AG_NEWS(split="train")

# Define tokenizer and yield_tokens
tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text.lower())  # Lowercase conversion for consistency

# Build vocabulary
vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

# Print the vocabulary size and sample tokens
print(f"Vocabulary size: {len(vocab)}")
print(f"Sample tokens: {list(vocab.get_stoi().keys())[:10]}")

# Here are some token indices:
vocab(["age","hello"])


# Dataset
# You can convert the dataset into map-style datasets and then perform a random split to create separate training
# and validation datasets. The training dataset will contain 95% of the samples, while the validation dataset will
# contain the remaining 5%. These datasets can be used for training and evaluating a machine learning model for text classification on the AG_NEWS dataset.
# Split the dataset into training and testing iterators.
train_iter, test_iter = AG_NEWS()

# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device


# Data loader
# Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to
# process the raw data strings from the dataset iterators.
# The function text_pipeline will tokenize the input text, and vocab will then be applied to get the token indices.
# The label_pipeline will ensure that the labels start at zero.
def text_pipeline(x):
  return vocab(tokenizer(x))

def label_pipeline(x):
   return int(x) – 1


# The provided code defines a collate_batch function in PyTorch, which is used with data loaders to customize
# batch creation from individual samples
def collate_batch(batch):
    label_list, text_list, offsets = [], [], [0]
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device), offsets.to(device)
#Convert the dataset objects to a data loader by applying the collate function.
BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
#You can observe the output sequence when you have the label, text, and offsets for each batch.
label, text, offsets=next(iter(valid_dataloader ))
label, text, offsets



# Neural network
# You have created a neural network for a text classification model using an EmbeddingBag layer, followed by a softmax
# output layer. Additionally, you have initialized the model using a specific method.
from torch import nn

class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text, offsets):
        embedded = self.embedding(text, offsets)
        return self.fc(embedded)
#You have created the model, and the embedding dimension size is a free parameter.
emsize=64

#You need the vocabulary size to determine the number of embeddings.
vocab_size=len(vocab)
vocab_size

#You have also determined the number of classes for the output layer.
num_class

#Creating the model:
model = TextClassificationModel(vocab_size, emsize, num_class).to(device)
model


# The code line predicted_label=model(text, offsets) is used to obtain predicted labels from a machine learning
# model for a given input text and its corresponding offsets. The model is the machine learning model being used
# for text classification or similar tasks.
predicted_label=model(text, offsets)
# Now, verify the output shape of your model. In this case, the model is trained with a mini-batch size of 64 samples.
# The output layer of the model produces four logits for each neuron, corresponding to the four classes in the
# classification task. You can also create a function to find the accuracy given a dataset.
predicted_label.shape
# torch.Size([64, 4])
#
# Function predict takes in a text and a text pipeline, which preprocesses the text for machine learning. It uses a
# pre-trained model to predict the label of the text for text classification on the AG_NEWS dataset.
# The function returns the predicted label as a result.
def predict(text, text_pipeline):
    with torch.no_grad():
        text = torch.tensor(text_pipeline(text))
        output = model(text, torch.tensor([0]))
        return ag_news_label[output.argmax(1).item() + 1]
predict("I like sports",text_pipeline )
# 'Sci/Tec'
#
# Create a function to evaluate the model's accuracy on a dataset.
def evaluate(dataloader):
    model.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for idx, (label, text, offsets) in enumerate(dataloader):
            predicted_label = model(text, offsets)

            total_acc += (predicted_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count
# The model was evaluated, and it was found that its performance is no better than average. This outcome is expected,
# considering that the model has not undergone any training yet.
evaluate(test_dataloader)


# Train the Model
LR=0.1

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
#Training the model, which should take about 20 minutes.
EPOCHS = 10
cum_loss_list=[]
acc_epoch=[]
acc_old=0

for epoch in tqdm(range(1, EPOCHS + 1)):
    model.train()
    cum_loss=0
    for idx, (label, text, offsets) in enumerate(train_dataloader):
        optimizer.zero_grad()
        predicted_label = model(text, offsets)
        loss = criterion(predicted_label, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        cum_loss+=loss.item()

    cum_loss_list.append(cum_loss)
    accu_val = evaluate(valid_dataloader)
    acc_epoch.append(accu_val)

    if accu_val > acc_old:
      acc_old= accu_val
      torch.save(model.state_dict(), 'my_model.pth')

#You can plot the cost and accuracy for each epoch. You'll see that with just ten epochs, you achieve an accuracy
# of over 80% on the validation data. You can increase the number of epochs to observe further results.
plot(cum_loss_list,acc_epoch)

evaluate(test_dataloader)


# This code snippet provides a summary for generating a 3D t-SNE visualization of embeddings using Plotly.
# It demonstrates how words that are similar to each other are positioned closer together.
# Get the first batch from the validation data
batch = next(iter(valid_dataloader))

# Extract the text and offsets from the batch
label, text, offsets = batch

# Send the data to the device (GPU if available)
text = text.to(device)
offsets = offsets.to(device)

# Get the embeddings bag output for the batch
embedded = model.embedding(text, offsets)

# Convert the embeddings tensor to a numpy array
embeddings_numpy = embedded.detach().cpu().numpy()

# Perform t-SNE on the embeddings to reduce their dimensionality to 3D.
X_embedded_3d = TSNE(n_components=3).fit_transform(embeddings_numpy)

# Create a 3D scatter plot using Plotly
trace = go.Scatter3d(
    x=X_embedded_3d[:, 0],
    y=X_embedded_3d[:, 1],
    z=X_embedded_3d[:, 2],
    mode='markers',
    marker=dict(
        size=5,
        color=label.numpy(),  # Use label information for color
        colorscale='Viridis',  # Choose a colorscale
        opacity=0.8
    )
)

layout = go.Layout(title="3D t-SNE Visualization of Embeddings",
                   scene=dict(xaxis_title='Dimension 1',
                              yaxis_title='Dimension 2',
                              zaxis_title='Dimension 3'))

fig = go.Figure(data=[trace], layout=layout)
fig.show()

#You can make a prediction on the following article using the function predict.
article="""Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, 
coming from behind to claim a vital 2-1 victory at the Women’s World Cup.
Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – 
as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular 
Stadium in Australia.
Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get 
a clean connection on a clearance with the resulting contact squirming into her own net to level the score.
Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, 
slotting home from the edge of the area to seal the three points."""
result = predict(article, text_pipeline)

markdown_content = f'''
<div style="background-color: lightgray; padding: 10px;">
    <h3>{article}</h3>
    <h4>The category of the news article: {result}</h4>
</div>
'''

md(markdown_content)



# - Load the pre-trained model (path = 'my_model.pth').
model.load_state_dict(torch.load('my_model.pth'))
model.eval()
# TextClassificationModel(
#   (embedding): EmbeddingBag(95811, 64, mode=mean)
#   (fc): Linear(in_features=64, out_features=4, bias=True)
# )
#
# Define the list of new articles for classification
new_articles = [
    "International talks have made significant headway with the signing of a climate accord that commits countries to reduce emissions by 40% over the next two decades. World leaders expressed optimism at the conclusion of the summit.",
    "In a stunning upset, the underdog team won the national title, beating the favorites in a match that featured an incredible comeback and a last-minute goal that sealed their victory in front of a record crowd.",
    "Market analysts are optimistic as the tech startup's stock prices soared after the announcement of their latest product, which promises to revolutionize how we interact with smart devices.",
    "A recent study published in a leading scientific journal suggests that a new drug has shown promise in the treatment of Alzheimer's disease, outperforming current leading medications in early clinical trials.",
    "Diplomatic relations have taken a positive turn with the recent peace talks that aim to end decades of conflict. The ceasefire agreement has been welcomed by the international community.",
    "Economic indicators show a sharp rebound in manufacturing, with the automobile industry leading the charge. Analysts predict this surge will result in significant job creation over the next year.",
    "Researchers at the university's astrophysics department have discovered a potentially habitable exoplanet. The planet, which lies in a nearby star system, has conditions that could support liquid water and, possibly, life.",
    "The sports world is in shock as a legendary player announces their retirement. Over an illustrious 20-year career, the athlete has amassed numerous records and is regarded as one of the greatest to ever play the game.",
    "A multinational corporation has announced a major investment in renewable energy. The initiative includes the construction of new wind farms and solar panels that will power hundreds of thousands of homes.",
    "Climate scientists warn that the melting of the polar ice caps has been accelerating at an alarming rate, raising sea levels and threatening coastal cities worldwide with increased flooding risks."
]
# Classify each article and display the results.
for i, article in enumerate(new_articles, start=1):
    prediction = predict(article, text_pipeline)
    print(f"Article {i} is classified as: {prediction}\n")