# !pip install -q numpy pandas matplotlib seaborn scikit-learn
# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install portalocker==2.8.2
# !pip install torchdata==0.7.1
# !pip install nltk
# !pip install spacy
# !python -m spacy download en_core_web_sm
# !python -m spacy download de_core_news_sm

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.datasets import multi30k, Multi30k
from typing import Iterable, List
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
from torchdata.datapipes.iter import IterableWrapper, Mapper
import torchtext
from torchtext.vocab import build_vocab_from_iterator
from nltk.translate.bleu_score import sentence_bleu
import torch
import torch.nn as nn
import torch.optim as optim


import numpy as np
import random
import math
import time
from tqdm import tqdm
import matplotlib.pyplot as plt


# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Sequence-to-sequence (Seq2seq) models have revolutionized various natural language processing (NLP) tasks,
# such as machine translation, text summarization, and chatbots. These models employ Recurrent Neural Networks (RNNs)
# to process variable-length input sequences and generate variable-length output sequences.

# RNNs are a class of neural networks designed to process sequential data. They maintain an internal memory() to
# capture information from previous steps and use it for current predictions. RNNs have a recurrent connection
# that allows information to flow from one step to the next. Recurrent Neural Networks (RNNs) operate on sequences
# and utilize previous states to influence the current state.

W_xh=torch.tensor(-10.0)
W_hh=torch.tensor(10.0)
b_h=torch.tensor(0.0)
x_t=1
h_prev=torch.tensor(-1)

X=[1,1,-1,-1,1,1]
# Assuming that the intial state , with the above input vector , the state vector  should look like this:
H=[-1,-1,0,1,0,-1]
# Initialize an empty list to store the predicted state values
H_hat = []
# Loop through each data point in the input sequence X
t=1
for x in X:
    # Assign the current data point to x_t
    print("t=",t)
    x_t = x
    # Print the value of the previous state (h at time t-1)
    print("h_t-1", h_prev.item())

    # Compute the current state (h at time t) using the RNN formula with tanh activation
    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)

    # Update h_prev to the current state value for the next iteration
    h_prev = h_t

    # Print the current input value (x at time t)
    print("x_t", x_t)

    # Print the computed state value (h at time t)
    print("h_t", h_t.item())
    print("\n")

    # Append the current state value to the H_hat list after converting it to integer
    H_hat.append(int(h_t.item()))
    t+=1
# t= 1
# h_t-1 -1
# x_t 1
# h_t -1.0
#
#
# t= 2
# h_t-1 -1.0
# x_t 1
# h_t -1.0
#
#
# t= 3
# h_t-1 -1.0
# x_t -1
# h_t 0.0
#
#
# t= 4
# h_t-1 0.0
# x_t -1
# h_t 1.0
#
#
# t= 5
# h_t-1 1.0
# x_t 1
# h_t 0.0
#
#
# t= 6
# h_t-1 0.0
# x_t 1
# h_t -1.0
#
# You can evaluate the accuracy of the predicted state H_hat by comparing it to the actual state H. In RNNs, the state
# is utilized to predict an output sequence  based on the given input sequence .
H_hat
# [-1, -1, 0, 1, 0, -1]

H
#[-1, -1, 0, 1, 0, -1]


# Note: When using an LSTM, you have an additional cell state. However, if you were using a GRU, you would only have the hidden state.
class Encoder(nn.Module):
    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):
        super().__init__()

        self.hid_dim = hid_dim
        self.n_layers = n_layers

        self.embedding = nn.Embedding(vocab_len, emb_dim)

        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, input_batch):
        #input_batch = [src len, batch size]
        embed = self.dropout(self.embedding(input_batch))
        embed = embed.to(device)
        #outputs = [src len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]
        outputs, (hidden, cell) = self.lstm(embed)

        return hidden, cell
# create an encoder instance to see how it works:
vocab_len = 8
emb_dim = 10
hid_dim=8
n_layers=1
dropout_prob=0.5
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)
#see a simple example where the encoder forward method transforms the src sentence into a hidden and cell states.
# tensor([[0],[3],[4],[2],[1]]) is equal to src = 0,3,4,2,1 in which each number represents a token in the src
# vocabulary. For instance, 0:<bos>,3:"Das", 4:"ist",2:"sch√∂n", 1:<eos>. Note that here you have batch size of 1.

src_batch = torch.tensor([[0,3,4,2,1]])
# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default
src_batch = src_batch.t().to(device)
print("Shape of input(src) tensor:", src_batch.shape)
hidden_t , cell_t = encoder_t(src_batch)
print("Hidden tensor from encoder:",hidden_t ,"\nCell tensor from encoder:", cell_t)
# #Shape of input(src) tensor: torch.Size([5, 1])
# Hidden tensor from encoder: tensor([[[ 0.0880,  0.0905, -0.2786,  0.0094,  0.0259,  0.1236, -0.0018,
#           -0.1118]]], grad_fn=<StackBackward0>)
# Cell tensor from encoder: tensor([[[ 0.5130,  0.2259, -0.5699,  0.0245,  0.0676,  0.2539, -0.0028,
#           -0.3372]]], grad_fn=<StackBackward0>)


class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers


        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):


        #input = [batch size]

        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]

        #n directions in the decoder will both always be 1, therefore:
        #hidden = [n layers, batch size, hid dim]
        #context = [n layers, batch size, hid dim]

        input = input.unsqueeze(0)
        #input = [1, batch size]

        embedded = self.dropout(self.embedding(input))
        #embedded = [1, batch size, emb dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        #output = [seq len, batch size, hid dim * n directions]
        #hidden = [n layers * n directions, batch size, hid dim]
        #cell = [n layers * n directions, batch size, hid dim]

        #seq len and n directions will always be 1 in the decoder, therefore:
        #output = [1, batch size, hid dim]
        #hidden = [n layers, batch size, hid dim]
        #cell = [n layers, batch size, hid dim]
        prediction_logit = self.fc_out(output.squeeze(0))
        prediction = self.softmax(prediction_logit)
        #prediction = [batch size, output dim]


        return prediction, hidden, cell

#create a decoder instance. The output dimension is set as the target vocab length.
output_dim = 6
emb_dim=10
hid_dim = 8
n_layers=1
dropout=0.5
decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)

input_t = torch.tensor([0]).to(device) #<bos>
input_t.shape
prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)
print("Prediction:", prediction, '\nHidden:',hidden,'\nCell:', cell)
# Prediction: tensor([[-1.5424, -1.8199, -1.7506, -2.0357, -1.9518, -1.7271]],
#        grad_fn=<LogSoftmaxBackward0>)
# Hidden: tensor([[[ 0.0769, -0.0700, -0.1569,  0.1378,  0.2742,  0.2046,  0.1736,
#           -0.0118]]], grad_fn=<StackBackward0>)
# Cell: tensor([[[ 0.2315, -0.1519, -0.3135,  0.2963,  0.3473,  0.3114,  0.3145,
#           -0.0273]]], grad_fn=<StackBackward0>)

# Encoder-decoder connection
# Alright! You learned how to create encoder and decoder modules and how to pass input to them. Now you need to create the connection so that the model can process (src,trg) pairs and generate the translation. suppose that trg is tensor ([[0],[2],[3],[5],[1]]) which is equal to sequence 0,2,3,5,1 in which each number represents a token in the target vocabulary. For instance, 0:<bos>,2:"this", 3:"is",5:"beautiful", 1:<eos>.

#trg = [trg len, batch size]
#teacher_forcing_ratio is probability to use teacher forcing
#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time
teacher_forcing_ratio = 0.5
trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)


batch_size = trg.shape[1]
trg_len = trg.shape[0]
trg_vocab_size = decoder_t.output_dim

#tensor to store decoder outputs
outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)

#send to device

hidden_t = hidden_t.to(device)
cell_t = cell_t.to(device)


#first input to the decoder is the <bos> tokens
input = trg[0,:]


for t in range(1, trg_len):

    #you loop through the trg len and generate tokens
    #decoder receives previous generated token, cell and hidden
    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell
    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)

    #place predictions in a tensor holding predictions for each token
    outputs_t[t] = output_t

    #decide if you are going to use teacher forcing or not
    teacher_force = random.random() < teacher_forcing_ratio

    #get the highest predicted token from your predictions
    top1 = output_t.argmax(1)


    #if teacher forcing, use actual next token as next input
    #if not, use predicted token
    #input = trg[t] if teacher_force else top1
    input = trg[t] if teacher_force else top1

print(outputs_t,outputs_t.shape )
# tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],
#
#         [[-1.5735, -1.8341, -1.7657, -1.9467, -2.0164, -1.6820]],
#
#         [[-1.5548, -1.8480, -1.7114, -1.9743, -1.9445, -1.7798]],
#
#         [[-1.5119, -1.7303, -1.7401, -2.1232, -1.7153, -2.0618]],
#
#         [[-1.5248, -1.6579, -1.7591, -2.1937, -1.7129, -2.0576]]],
#        grad_fn=<CopySlices>) torch.Size([5, 1, 6])
#
# The size of output tensor is (trg_len, batch_size, trg_vocab_size). This is because for each trg token
# (length of trg) the model outputs a probability distribution over all possible tokens(trg vocab length).
# Therefore, to generate the predicted tokens or translation of the src sentence, you need to get the maximum
# probability for each token:
# # Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors
pred_tokens = outputs_t.argmax(2)
print(pred_tokens)
# tensor([[0],
#         [0],
#         [0],
#         [0],
#         [0]])



# Sequence-to-sequence model implementation in PyTorch
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device,trg_vocab):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.trg_vocab = trg_vocab

        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must be equal!"
        assert encoder.n_layers == decoder.n_layers, \
            "Encoder and decoder must have equal number of layers!"

    def forward(self, src, trg, teacher_forcing_ratio = 0.5):
        #src = [src len, batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time


        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim

        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)

        #last hidden state of the encoder is used as the initial hidden state of the decoder
        hidden, cell = self.encoder(src)
        hidden = hidden.to(device)
        cell = cell.to(device)


        #first input to the decoder is the <bos> tokens
        input = trg[0,:]

        for t in range(1, trg_len):

            #insert input token embedding, previous hidden and previous cell states
            #receive output tensor (predictions) and new hidden and cell states
            output, hidden, cell = self.decoder(input, hidden, cell)

            #place predictions in a tensor holding predictions for each token
            outputs[t] = output

            #decide if you are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio

            #get the highest predicted token from your predictions
            top1 = output.argmax(1)


            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            #input = trg[t] if teacher_force else top1
            input = trg[t] if teacher_force else top1


        return outputs

def train(model, iterator, optimizer, criterion, clip):

    model.train()

    epoch_loss = 0

    # Wrap iterator with tqdm for progress logging
    train_iterator = tqdm(iterator, desc="Training", leave=False)

    for i, (src,trg) in enumerate(iterator):

        src = src.to(device)
        trg = trg.to(device)
        optimizer.zero_grad()

        output = model(src, trg)

        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)

        trg = trg[1:].contiguous().view(-1)

        #trg = [(trg len - 1) * batch size]
        #output = [(trg len - 1) * batch size, output dim]

        loss = criterion(output, trg)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        # Update tqdm progress bar with the current loss
        train_iterator.set_postfix(loss=loss.item())

        epoch_loss += loss.item()


    return epoch_loss / len(list(iterator))


def evaluate(model, iterator, criterion):

    model.eval()

    epoch_loss = 0

    # Wrap iterator with tqdm for progress logging
    valid_iterator = tqdm(iterator, desc="Training", leave=False)

    with torch.no_grad():

        for i, (src,trg) in enumerate(iterator):

            src = src.to(device)
            trg = trg.to(device)

            output = model(src, trg, 0) #turn off teacher forcing

            #trg = [trg len, batch size]
            #output = [trg len, batch size, output dim]

            output_dim = output.shape[-1]

            output = output[1:].view(-1, output_dim)

            trg = trg[1:].contiguous().view(-1)


            #trg = [(trg len - 1) * batch size]
            #output = [(trg len - 1) * batch size, output dim]

            loss = criterion(output, trg)
            # Update tqdm progress bar with the current loss
            valid_iterator.set_postfix(loss=loss.item())

            epoch_loss += loss.item()

    return epoch_loss / len(list(iterator))


# A "Multi30K_de_en_dataloader.py" file has been created that contains all the transformation processes on data.
# Here, you only download the file:
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'
# --2025-09-24 00:23:28--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py
# 169.63.118.104ourses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)...
# Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
# 200 OKequest sent, awaiting response...
# Length: 4890 (4.8K) [application/x-python]
# Saving to: ‚ÄòMulti30K_de_en_dataloader.py‚Äô
#
# Multi30K_de_en_data 100%[===================>]   4.78K  --.-KB/s    in 0s
#
# 2025-09-24 00:23:28 (610 MB/s) - ‚ÄòMulti30K_de_en_dataloader.py‚Äô saved [4890/4890]
#
# Let's run it:
%run Multi30K_de_en_dataloader.py
# There you go! You only need to call the function get_translation_dataloaders(batch_size = N,flip=True)
# with an arbitrary batch size N and setting flip to True in order for the LSTM encoder receive input sequence
# in reversed order. This can help the training.
train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)
# You can check the src and trg tensors:
src, trg = next(iter(train_dataloader))
src,trg
# (tensor([[    2,     2,     2,     2],
#          [    3,  5510,  5510, 12642],
#          [    1,     3,     3,     8],
#          [    1,     1,     1,  1701],
#          [    1,     1,     1,     3]]),
#  tensor([[   2,    2,    2,    2],
#          [   3, 6650,  216,    6],
#          [   1, 4623,  110, 3398],
#          [   1,  259, 3913,  202],
#          [   1,  172, 1650,  109],
#          [   1, 9953, 3823,   37],
#          [   1,  115,   71,    3],
#          [   1,  692, 2808,    1],
#          [   1, 3428, 2187,    1],
#          [   1,    5,    5,    1],
#          [   1,    3,    3,    1]]))
#
# You can also get the english and german strings using index_to_eng and index_to_german functions provided
# in the .py file:
data_itr = iter(train_dataloader)
# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)
for n in range(1000):
    german, english= next(data_itr)

for n in range(3):
    german, english=next(data_itr)
    german=german.T
    english=english.T
    print("________________")
    print("german")
    for g in german:
        print(index_to_german(g))
    print("________________")
    print("english")
    for e in english:
        print(index_to_eng(e))
# german
# <bos> Personen mit schwarzen H√ºten in der Innenstadt . <eos>
# <bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>
# <bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>
# <bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>
# ________________
# english
# <bos> People in black hats gathered together downtown . <eos> <pad> <pad> <pad>
# <bos> A group of people protesting in a city . <eos> <pad> <pad>
# <bos> A group is letting their political opinion be known . <eos> <pad>
# <bos> A group of people are sitting on a rocky beach . <eos>
# ________________
# german
# <bos> Zwei sitzende Personen mit H√ºten und Sonnenbrillen . <eos>
# <bos> Ein kleiner Junge mit Hut beim Angeln . <eos>
# <bos> Diese zwei Frauen haben Spa√ü im Giorgio's . <eos>
# <bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>
# ________________
# english
# <bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>
# <bos> A young boy in a hat is fishing by himself . <eos>
# <bos> These two women is at Giorgio 's having fun . <eos> <pad>
# <bos> Two young children are asleep on a couch . <eos> <pad> <pad>
# ________________
# german
# <bos> Zwei junge M√§dchen marschieren in einem Umzug . <eos>
# <bos> Eine Frau l√§uft vor einer gestreiften Wand . <eos>
# <bos> Ein Mann f√§hrt Jet-Ski auf dem Ozean . <eos>
# <bos> Die st√§dtischen Stra√üenbahnen an einem sonnigen Tag . <eos>
# ________________
# english
# <bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad> <pad>
# <bos> A woman is running in front of a striped wall . <eos> <pad>
# <bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>
# <bos> The urban trolly 's of a city on a sunny day . <eos>


# This code sets the random seed for various libraries and modules. This is done to make the results reproducible:
SEED = 1234
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

INPUT_DIM = len(vocab_transform['de'])
OUTPUT_DIM = len(vocab_transform['en'])
ENC_EMB_DIM = 128 #256
DEC_EMB_DIM = 128 #256
HID_DIM = 256 #512
N_LAYERS = 1 #2
ENC_DROPOUT = 0.3 #0.5
DEC_DROPOUT = 0.3 #0.5

enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)

model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)


def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)

model.apply(init_weights)
# Seq2Seq(
#   (encoder): Encoder(
#     (embedding): Embedding(19214, 128)
#     (lstm): LSTM(128, 256, dropout=0.3)
#     (dropout): Dropout(p=0.3, inplace=False)
#   )
#   (decoder): Decoder(
#     (embedding): Embedding(10837, 128)
#     (lstm): LSTM(128, 256, dropout=0.3)
#     (fc_out): Linear(in_features=256, out_features=10837, bias=True)
#     (softmax): LogSoftmax(dim=1)
#     (dropout): Dropout(p=0.3, inplace=False)
#   )
#   (trg_vocab): Vocab()
# )


# This code defines a function count_parameters that counts the number of trainable parameters in a given model.
# It then prints the count of trainable parameters in a formatted string.
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')
#The model has 7,422,165 trainable parameters


optimizer = optim.Adam(model.parameters())

PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']

criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)
#The following helper function provides a convenient way to calculate the elapsed time in minutes and seconds given
# the start and end times. It will be used to measure the time taken for each epoch during training or any other
# time-related calculations.
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs
#start the training epochs:
#can uncomment and execute this code in the labs supporting CUDA environment.

# torch.cuda.empty_cache()

# N_EPOCHS = 3 #run the training for at least 5 epochs
# CLIP = 1

# best_valid_loss = float('inf')
# best_train_loss = float('inf')
# train_losses = []
# valid_losses = []

# train_PPLs = []
# valid_PPLs = []

# for epoch in range(N_EPOCHS):

#     start_time = time.time()

#     train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)
#     train_ppl = math.exp(train_loss)
#     valid_loss = evaluate(model, valid_dataloader, criterion)
#     valid_ppl = math.exp(valid_loss)


#     end_time = time.time()

#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)


#     if valid_loss < best_valid_loss:

#         best_valid_loss = valid_loss
#         torch.save(model.state_dict(), 'RNN-TR-model.pt')

#     train_losses.append(train_loss)
#     train_PPLs.append(train_ppl)
#     valid_losses.append(valid_loss)
#     valid_PPLs.append(valid_ppl)

#     print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
#     print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')
#     print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')
# visualize the model train and validation losses over the training epochs:
# import matplotlib.pyplot as plt

# # Create a list of epoch numbers
# epochs = [epoch+1 for epoch in range(N_EPOCHS)]

# # Create the figure and axes
# fig, ax1 = plt.subplots(figsize=(10, 6))
# ax2 = ax1.twinx()

# # Plotting the training and validation loss
# ax1.plot(epochs, train_losses, label='Train Loss', color='blue')
# ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')
# ax1.set_xlabel('Epochs')
# ax1.set_ylabel('Loss')
# ax1.set_title('Training and Validation Loss/PPL')

# # Plotting the training and validation perplexity
# ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')
# ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')
# ax2.set_ylabel('Perplexity')

# # Adjust the y-axis scaling for PPL plot
# ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)

# # Set the legend
# lines1, labels1 = ax1.get_legend_handles_labels()
# lines2, labels2 = ax2.get_legend_handles_labels()
# lines = lines1 + lines2
# labels = labels1 + labels2
# ax1.legend(lines, labels, loc='upper right')


# # Show the plot
# plt.show()


# Loading the saved model
# If you want to skip training and load the pre-trained model instead, run the following cell:
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'
model.load_state_dict(torch.load('RNN-TR-model.pt',map_location=torch.device('cpu')))
# --2025-09-24 00:27:28--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt
# Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104
# Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
# 200 OKequest sent, awaiting response...
# Length: 29692282 (28M) [binary/octet-stream]
# Saving to: ‚ÄòRNN-TR-model.pt‚Äô
#
# RNN-TR-model.pt     100%[===================>]  28.32M  66.0MB/s    in 0.4s
#
# 2025-09-24 00:27:29 (66.0 MB/s) - ‚ÄòRNN-TR-model.pt‚Äô saved [29692282/29692282]
#
# <All keys matched successfully>
#
# Model inference
# Next, create a generator function that generates translations for input source sentences:
import torch.nn.functional as F

def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):
    model.eval()  # Set the model to evaluation mode

    with torch.no_grad():
        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)

        # Pass the source tensor through the encoder
        hidden, cell = model.encoder(src_tensor)

        # Create a tensor to store the generated translation
        # get_stoi() maps tokens to indices
        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token

        # Convert the initial token to a PyTorch tensor
        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension

        # Move the tensor to the same device as the model
        trg_tensor = trg_tensor.to(model.device)


        # Generate the translation
        for _ in range(max_len):

            # Pass the target tensor and the previous hidden and cell states through the decoder
            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)

            # Get the predicted next token
            pred_token = output.argmax(1)[-1].item()

            # Append the predicted token to the translation
            trg_indexes.append(pred_token)


            # If the predicted token is the <eos> token, stop generating
            if pred_token == trg_vocab.get_stoi()['<eos>']:
                break

            # Convert the predicted token to a PyTorch tensor
            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension

            # Move the tensor to the same device as the model
            trg_tensor = trg_tensor.to(model.device)

        # Convert the generated tokens to text
        # get_itos() maps indices to tokens
        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]

        # Remove the <sos> and <eos> from the translation
        if trg_tokens[0] == '<bos>':
            trg_tokens = trg_tokens[1:]
        if trg_tokens[-1] == '<eos>':
            trg_tokens = trg_tokens[:-1]

        # Return the translation list as a string

        translation = " ".join(trg_tokens)

        return translation

#check the model's output for a sample sentence:
# model.load_state_dict(torch.load('RNN-TR-model.pt'))

# Actual translation: Asian man sweeping the walkway.
src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'


generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)
#generated_translation = " ".join(generated_translation_list).replace("<bos>", "").replace("<eos>", "")
print(generated_translation)
#An Asian man is on the sidewalk


# BLEU score metric for evaluation
# While peplexity serves as a general metric to evaluate the performance of language model in predicting the
# correct next token, BLEU score is helpful in evaluating the quality of the final generated translation.
# Validating the results using BLEU score is helpful when there is more than a single valid translation for a
# sentence as you can include many translation versions in the reference list and compare the generated translation
# with different versions of translations.
# The BLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of
# machine-generated translations by comparing them to one or more reference translations. It measures the
# similarity between the generated translation and the reference translations based on n-gram matching.

def calculate_bleu_score(generated_translation, reference_translations):
    # Convert the generated translations and reference translations into the expected format for sentence_bleu
    references = [reference.split() for reference in reference_translations]
    hypothesis = generated_translation.split()

    # Calculate the BLEU score
    bleu_score = sentence_bleu(references, hypothesis)

    return bleu_score

#calculate the BLEU score for a sample sentence:
reference_translations = [
    "Asian man sweeping the walkway .",
    "An asian man sweeping the walkway .",
    "An Asian man sweeps the sidewalk .",
    "An Asian man is sweeping the sidewalk .",
    "An asian man is sweeping the walkway .",
    "Asian man sweeping the sidewalk ."
]

bleu_score = calculate_bleu_score(generated_translation, reference_translations)
print("BLEU Score:", bleu_score)
#BLEU Score: 0.5

#Translate a German sentence to English
german_text = "Menschen gehen auf der Stra√üe"

# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.
english_translation = generate_translation(
    model,
    src_sentence=german_text,
    src_vocab=vocab_transform['de'],
    trg_vocab=vocab_transform['en'],
    max_len=50
)

# Display the original and translated text
print(f"Original German text: {german_text}")
print(f"Translated English text: {english_translation}")
#Original German text: Menschen gehen auf der Stra√üe
#Translated English text: People are walking on the street .
