# # All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# # !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 torch=2.1.0+cu118
# # - Update a specific package
# # !pip install pmdarima -U
# # - Update a package to specific version
# # !pip install --upgrade pmdarima==2.0.2
# # Note: If your environment doesn't support "!pip install", use "!mamba install"
# !pip install torch==2.2.2
# !pip install torchtext==0.17.2
# !pip install portalocker==2.8.2
# !pip install torchdata==0.7.1
# !pip install pandas
# !pip install matplotlib==3.9.0 scikit-learn==1.5.0
# !pip install numpy==1.26.0
# !pip install --user transformers==4.42.1
# !pip install --user datasets # 2.20.0
# #!pip install portalocker>=2.0.0
# #!pip install torch==2.3.1
# !pip install --user torchmetrics==1.4.0.post0
# #!pip install numpy==1.26.4
# #!pip install peft==0.11.1
# #!pip install evaluate==0.4.2
# #!pip install -q bitsandbytes==0.43.1
# !pip install --user accelerate==0.31.0
# !pip install --user torchvision==0.18.1
#
#
# !pip install --user trl==0.9.4
# !pip install --user protobuf==3.20.*
# #!pip install matplotlib
#
# !pip install torchmetrics
# Importing required libraries
# It is recommended that you import all required libraries in one place (here):
# •	Note: If you get an error after running the cell below, try restarting the Kernel, as some packages need a restart to be effective.

import torch
from torchmetrics import Accuracy
from torch.optim.lr_scheduler import LambdaLR
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import AutoConfig,AutoModelForCausalLM,AutoModelForSequenceClassification,BertConfig,BertForMaskedLM,TrainingArguments, Trainer, TrainingArguments
from transformers import AutoTokenizer,BertTokenizerFast,TextDataset,DataCollatorForLanguageModeling
from transformers import pipeline
from datasets import load_dataset
from trl import SFTConfig,SFTTrainer, DataCollatorForCompletionOnlyLM


#import numpy as np
#import pandas as pd
from tqdm.auto import tqdm
import math
import time
import matplotlib.pyplot as plt
#import pandas as pd


# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Supervised Fine-tuning with Pytorch
# Fine-tuning Transformers, specifically BERT (Bidirectional Encoder Representations from Transformers),
# refers to the process of training a pretrained BERT model on a specific downstream task.
# BERT is an encoder-only language model that has been pretrained on a large corpus of text to learn
# contextual representations of words.

#Let's start by loading the yelp_review data:
dataset = load_dataset("yelp_review_full")
dataset


# Let's check a sample record of the dataset:
dataset["train"][100]
#{'label': 0,
# 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}

#the label is the key of the class label
dataset["train"][100]["label"]
#0
#there is also the text
dataset["train"][100]['text']
#'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\"serving off their orders\\" when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'

#You can select a portion of data to decrease the training time:
dataset["train"] = dataset["train"].select([i for i in range(1000)])
dataset["test"] = dataset["test"].select([i for i in range(200)])
#There are two data fields:
#•	label: the label for the review
#•	text: a string containing the body of the user review

#Tokenizing data
#The next step is to load a BERT tokenizer to tokenize, pad and truncate reviews to handle variable-length sequences:
# Instantiate a tokenizer using the BERT base cased model
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Define a function to tokenize examples
def tokenize_function(examples):
    # Tokenize the text using the tokenizer
    # Apply padding to ensure all sequences have the same length
    # Apply truncation to limit the maximum sequence length
    return tokenizer(examples["text"], padding="max_length", truncation=True)


# Apply the tokenize function to the dataset in batches
tokenized_datasets = dataset.map(tokenize_function, batched=True)

#The keys in each element of tokenized_datasets are 'label', 'text', 'input_ids', 'token_type_ids', and 'attention_mask'.

tokenized_datasets['train'][0].keys()
#dict_keys(['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'])

#To apply the preprocessing function over the entire dataset, let's use the map method.
# You can speed up the map function by setting batched=True to process multiple elements of the dataset at once:
#Since the model is built on the PyTorch framework, it is crucial to prepare the dataset in a format that PyTorch
# can readily process. Follow these steps to ensure compatibility:
# Remove the text column because the model does not accept raw text as an input
tokenized_datasets = tokenized_datasets.remove_columns(["text"])

# Rename the label column to labels because the model expects the argument to be named labels
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# Set the format of the dataset to return PyTorch tensors instead of lists
tokenized_datasets.set_format("torch")

#the result is a set of tensors with the keys as:  'labels', 'input_ids', 'token_type_ids', 'attention_mask'
tokenized_datasets['train'][0].keys()
#dict_keys(['labels', 'input_ids', 'token_type_ids', 'attention_mask'])

DataLoader

#Next, create a DataLoader for train and test datasets so you can iterate over batches of data:
# Create a training data loader
train_dataloader = DataLoader(tokenized_datasets["train"], shuffle=True, batch_size=2)

# Create an evaluation data loader
eval_dataloader = DataLoader(tokenized_datasets["test"], batch_size=2)

#Train the model
#Load a pretrained model
# Instantiate a sequence classification model
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)

#Optimizer and learning rate schedule
#create an optimizer and learning rate scheduler to fine-tune the model. You can use the AdamW optimizer from PyTorch:
# Define the optimizer
optimizer = AdamW(model.parameters(), lr=5e-4)

# Set the number of epochs
num_epochs = 10

# Calculate the total number of training steps
num_training_steps = num_epochs * len(train_dataloader)

# Define the learning rate scheduler
lr_scheduler = LambdaLR(optimizer, lr_lambda=lambda current_step: (1 - current_step / num_training_steps))

#Check if CUDA is available and, then set the device accordingly.
# Check if CUDA is available and set the device accordingly
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Move the model to the appropriate device
model.to(device)
# BertForSequenceClassification(
#   (bert): BertModel(
#     (embeddings): BertEmbeddings(
#       (word_embeddings): Embedding(28996, 768, padding_idx=0)
#       (position_embeddings): Embedding(512, 768)
#       (token_type_embeddings): Embedding(2, 768)
#       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#       (dropout): Dropout(p=0.1, inplace=False)
#     )
#     (encoder): BertEncoder(
#       (layer): ModuleList(
#         (0-11): 12 x BertLayer(
#           (attention): BertAttention(
#             (self): BertSdpaSelfAttention(
#               (query): Linear(in_features=768, out_features=768, bias=True)
#               (key): Linear(in_features=768, out_features=768, bias=True)
#               (value): Linear(in_features=768, out_features=768, bias=True)
#               (dropout): Dropout(p=0.1, inplace=False)
#             )
#             (output): BertSelfOutput(
#               (dense): Linear(in_features=768, out_features=768, bias=True)
#               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#               (dropout): Dropout(p=0.1, inplace=False)
#             )
#           )
#           (intermediate): BertIntermediate(
#             (dense): Linear(in_features=768, out_features=3072, bias=True)
#             (intermediate_act_fn): GELUActivation()
#           )
#           (output): BertOutput(
#             (dense): Linear(in_features=3072, out_features=768, bias=True)
#             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#             (dropout): Dropout(p=0.1, inplace=False)
#           )
#         )
#       )
#     )
#     (pooler): BertPooler(
#       (dense): Linear(in_features=768, out_features=768, bias=True)
#       (activation): Tanh()
#     )
#   )
#   (dropout): Dropout(p=0.1, inplace=False)
#   (classifier): Linear(in_features=768, out_features=5, bias=True)
# )
#
# Training loop
# To keep track of training progress, let's use the "tqdm"
# library to add a progress bar over the number of training steps. The train_model function trains a model using a
# set of training data provided through a dataloader. It begins by setting up a progress bar to help monitor the
# training progress visually. The model is switched to training mode, which is necessary for certain model
# behaviors like dropout to work correctly during training. The function processes the data in batches for each epoch,
# which involves several steps for each batch: transferring the data to the correct device (like a GPU),
# running the data through the model to get outputs and calculate loss, updating the model's parameters using the
# calculated gradients, adjusting the learning rate, and clearing the old gradients. These steps are repeated for
# each batch of data, and the progress bar is updated accordingly to reflect the progress.
# Once all epochs are completed, the trained model is saved to be used later.
def train_model(model,tr_dataloader):

    # Create a progress bar to track the training progress
    progress_bar = tqdm(range(num_training_steps))

    # Set the model in training mode
    model.train()
    tr_losses=[]
    # Training loop
    for epoch in range(num_epochs):
        total_loss = 0
        # Iterate over the training data batches
        for batch in tr_dataloader:
            # Move the batch to the appropriate device
            batch = {k: v.to(device) for k, v in batch.items()}
            # Forward pass through the model
            outputs = model(**batch)
            # Compute the loss
            loss = outputs.loss
            # Backward pass (compute gradients)
            loss.backward()

            total_loss += loss.item()
            # Update the model parameters
            optimizer.step()
            # Update the learning rate scheduler
            lr_scheduler.step()
            # Clear the gradients
            optimizer.zero_grad()
            # Update the progress bar
            progress_bar.update(1)
        tr_losses.append(total_loss/len(tr_dataloader))
    #plot loss
    plt.plot(tr_losses)
    plt.title("Training loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.show()


#Evaluate
#The evaluate_model function works similarly to the train_model function but is used for evaluating the model's
# performance instead of training it. It uses a dataloader to process data in batches, setting the model to
# evaluation mode to ensure accuracy in measurements and disabling gradient calculations since it's not training.
# The function calculates predictions for each batch, updates an accuracy metric, and finally,
# prints the overall accuracy after processing all batches.

def evaluate_model(model, evl_dataloader):
    # Create an instance of the Accuracy metric for multiclass classification with 5 classes
    metric = Accuracy(task="multiclass", num_classes=5).to(device)

    # Set the model in evaluation mode
    model.eval()

    # Disable gradient calculation during evaluation
    with torch.no_grad():
        # Iterate over the evaluation data batches
        for batch in evl_dataloader:
            # Move the batch to the appropriate device
            batch = {k: v.to(device) for k, v in batch.items()}

            # Forward pass through the model
            outputs = model(**batch)

            # Get the predicted class labels
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)

            # Accumulate the predictions and labels for the metric
            metric(predictions, batch["labels"])

    # Compute the accuracy
    accuracy = metric.compute()

    # Print the accuracy
    print("Accuracy:", accuracy.item())

#can now train the model. This process will take a long time, and it is highly recommended that you do this only if
# you have the required resources. Please uncomment the following code to train the model.
# train_model(model=model,tr_dataloader=train_dataloader)

# torch.save(model, 'my_model.pt')

#Loading the saved model
#If you want to skip training and load the model that you trained for 10 epochs, go ahead and uncomment the following cell:
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/wFhKpkBMSgjmZKRSyayvsQ/bert-classification-model.pt'
model.load_state_dict(torch.load('bert-classification-model.pt',map_location=torch.device('cpu')))


#You can now evaluate the model. Please note that this process will take a while.
evaluate_model(model, eval_dataloader)
#Accuracy: 0.26499998569488525

#You are now ready to learn to tune a more complex model that can generate conversations between a human and an assistant using SFTtrainer.



#Training a conversational model using SFTTrainer
#The SFTTrainer from the trl (Transformers Reinforcement Learning) library is a tool used for supervised fine-tuning
# of language models. It helps refine pre-trained models using specific datasets to enhance their performance on
# targeted tasks.
#Objective
#Explore how fine-tuning a decoder transformer using a specific dataset affects the quality of the generated responses in a question-answering task.

#Step 1- Load the train split of "timdettmers/openassistant-guanaco" dataset from Hugging Face:
## Write your code here
dataset = load_dataset("timdettmers/openassistant-guanaco", split="train")
dataset[0]


#Step 2- Load the pretrained causal model "facebook/opt-350m" along with its tokenizer from Hugging Face:
## Write your code here
model = AutoModelForCausalLM.from_pretrained("facebook/opt-350m")
tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

#Step 3- Create instruction and response templates based on the train dataset format:
## Write your code here
instruction_template = "### Human:"
response_template = "### Assistant:"

#Step 4- Create a collator to curate data in the appropriate shape for training using **"DataCollatorForCompletionOnlyLM"**:
## Write your code here
collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)

#Step 5- Create an SFTTrainer object and pass the model as well as the dataset and collator:
## Write your code here
training_args = SFTConfig(
    output_dir="/tmp",
    num_train_epochs=10,
    #learning_rate=2e-5,
    save_strategy="epoch",
    fp16=True,
    per_device_train_batch_size=2,  # Reduce batch size
    per_device_eval_batch_size=2,  # Reduce batch size
    #gradient_accumulation_steps=4,  # Accumulate gradients
    max_seq_length=1024,
    do_eval=True
)

trainer = SFTTrainer(
    model,
    args=training_args,
    train_dataset=dataset,
    dataset_text_field="text",
    data_collator=collator,
)

#Step 6- Prompt the pretrained model with a specific question:
## Write your code here
pipe = pipeline("text-generation", model=model,tokenizer=tokenizer,max_new_tokens=70)
print(pipe('''Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0]["generated_text"])

#Step 6A (Optional)- Train the model:

## Write your code here
#trainer.train()
#Step 6B- Load the tuned model:
## Write your code here
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Assistant_model.pt'
model.load_state_dict(torch.load('Assistant_model.pt',map_location=torch.device('cpu')))

#Step 7- Check how the tuned model performs in answering the same specialized question:
## Write your code here
pipe = pipeline("text-generation", model=model,tokenizer=tokenizer,max_new_tokens=70)
print(pipe('''Can you write a short introduction about the relevance of the term "monopsony" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.''')[0]["generated_text"])
