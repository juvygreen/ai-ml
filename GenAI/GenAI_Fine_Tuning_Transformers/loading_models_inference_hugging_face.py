# Note: The package installation step may take a few minutes to complete. Please be patient and wait until the installation finishes before moving on to the next step. Do not interrupt the process or restart the kernel while the installation is in progress.
# %pip install -U \
#   "torch~=2.3.1" \
#   "torchvision~=0.18.0" \
#   "transformers~=4.40.0" \
#   --upgrade-strategy only-if-needed \
#   --prefer-binary
#
#
from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

#Text classification with DistilBERT
#First, let's initialize a tokenizer and a model for sentiment analysis using DistilBERT fine-tuned on the SST-2
# dataset. This setup is useful for tasks where you need to quickly classify the sentiment of a piece of
# text with a pretrained, efficient transformer model.
# Load the tokenizer and model

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
# tokenizer_config.json: 100%
#  48.0/48.0 [00:00<00:00, 6.00kB/s]
# vocab.txt: 100%
#  232k/232k [00:00<00:00, 15.4MB/s]
# config.json: 100%
#  629/629 [00:00<00:00, 105kB/s]
# model.safetensors: 100%
#  268M/268M [00:01<00:00, 189MB/s]
#
# Preprocess the input text
# Tokenize the input text and convert it to a format suitable for the model:
# Sample text
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")

print(inputs)
# {'input_ids': tensor([[  101, 23156,   999,  2017,  1005,  2310,  2180,  1037,  2489,  7281,
#           2000,  1996, 17094,  1012,  7514,  2663,  2000,  4366,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
# The token ids are the token indexes attention_mask is essential for correctly processing padded sequences,
# ensuring efficient computation, and maintaining model performance. Even when no tokens are explicitly masked,
# it helps the model differentiate between actual content and padding, which is critical for accurate and efficient
# processing of input data.
# Perform inference
# The torch.no_grad() context manager is used to disable gradient calculation. This reduces memory consumption and
# speeds up computation, as gradients are not needed for inference (i.e. when you are not training the model).
# The **inputs syntax is used to unpack a dictionary of keyword arguments in Python. In the context of
# the model(**inputs):
# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

#Another method is input_ids, and attention_mask is their own parameter.

#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])
#Get the logits
#The logits are the raw, unnormalized predictions of the model. Let's extract the logits from the model's outputs to
# perform further processing, such as determining the predicted class or calculating probabilities.

logits = outputs.logits
logits.shape
#torch.Size([1, 2])

#Post-process the output
#Convert the logits to probabilities and get the predicted class:

# Convert logits to probabilities
probs = torch.softmax(logits, dim=-1)

# Get the predicted class
predicted_class = torch.argmax(probs, dim=-1)

# Map the predicted class to the label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class]

print(f"Predicted label: {predicted_label}")
#Predicted label: POSITIVE


#Text generation with GPT-2
#Load tokenizer
#Load the pretrained GPT-2 tokenizer. The tokenizer is responsible for converting text into
# tokens that the model can understand.
# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# tokenizer_config.json: 100%
#  26.0/26.0 [00:00<00:00, 3.76kB/s]
# vocab.json: 100%
#  1.04M/1.04M [00:00<00:00, 52.6MB/s]
# merges.txt: 100%
#  456k/456k [00:00<00:00, 18.8MB/s]
# tokenizer.json: 100%
#  1.36M/1.36M [00:00<00:00, 72.9MB/s]
# config.json: 100%
#  665/665 [00:00<00:00, 105kB/s]
#
# Load the pretrained GPT-2 model with a language modeling head. The model generates text based on the input tokens.

# Load the tokenizer and model

model = GPT2LMHeadModel.from_pretrained("gpt2")
# model.safetensors: 100%
#  548M/548M [00:06<00:00, 83.7MB/s]
# generation_config.json: 100%
#  124/124 [00:00<00:00, 8.57kB/s]
#
# Preprocess the input text
# Tokenize the input text and convert it to a format suitable for the model, like before you have the token indexes, i.e., inputs.
# Prompt
prompt = "Once upon a time"

# Tokenize the input text
inputs = tokenizer(prompt, return_tensors="pt")
inputs
#{'input_ids': tensor([[7454, 2402,  257,  640]]), 'attention_mask': tensor([[1, 1, 1, 1]])}

# Perform inference
# Generate text using the model
# inputs: Input token IDs from the tokenizer
# attention_mask: Mask indicating which tokens to attend to
# pad_token_id:Padding token ID set to the end-of-sequence token ID
# max_length: Maximum length of the generated sequences
# num_return_sequence: Number of sequences to generate

# Generate text
output_ids = model.generate(
    inputs.input_ids,
    attention_mask=inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    max_length=50,
    num_return_sequences=1
)

output_ids
# tensor([[7454, 2402,  257,  640,   11,  262,  995,  373,  257, 1295,  286, 1049,
#          8737,  290, 1049, 3514,   13,  383,  995,  373,  257, 1295,  286, 1049,
#          3514,   11,  290,  262,  995,  373,  257, 1295,  286, 1049, 3514,   13,
#           383,  995,  373,  257, 1295,  286, 1049, 3514,   11,  290,  262,  995,
#           373,  257]])
#
# or

with torch.no_grad():
    outputs = model(**inputs)

outputs

#Post-process the output
#Decode the generated tokens to get the text:
# Decode the generated text
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(generated_text)

# Hugging Face pipeline() function
# The pipeline() function from the Hugging Face transformers library is a high-level API designed to simplify the
# usage of pretrained models for various natural language processing (NLP) tasks. It abstracts the complexities
# of model loading, tokenization, inference, and post-processing, allowing users to perform complex NLP tasks
# with just a few lines of code.
# Definition
# transformers.pipeline(
#     task: str,
#     model: Optional = None,
#     config: Optional = None,
#     tokenizer: Optional = None,
#     feature_extractor: Optional = None,
#     framework: Optional = None,
#     revision: str = 'main',
#     use_fast: bool = True,
#     model_kwargs: Dict[str, Any] = None,
#     **kwargs
# )

# Example 1: Text classification using pipeline()
# In this example, you will use the pipeline() function to perform text classification. You will load a pretrained
# text classification model and use it to classify a sample text.

# Load the text classification model:
# We initialize the pipeline for the text-classification task, specifying the model
# "distilbert-base-uncased-finetuned-sst-2-english". This model is fine-tuned for sentiment analysis.

# Classify the sample text:
# We use the classifier to classify a sample text: "Congratulations! You've won a free ticket to the Bahamas.
# Reply WIN to claim." The classifier function returns the classification result, which is then printed.
# # Load a general text classification model
# classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")
#
# Classify a sample text
result = classifier("Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.")
print(result)
#[{'label': 'POSITIVE', 'score': 0.9997586607933044}]


# Example 2: Language detection using pipeline()
# In this example, you will use the pipeline() function to perform language detection. You will load a pretrained
# language detection model and use it to identify the language of a sample text.

# Load the language detection model:
# We initialize the pipeline for the text-classification task, specifying the
# model "papluca/xlm-roberta-base-language-detection". This model is fine-tuned for language detection.

# Classify the sample text:
# We use the classifier to detect the language of a sample text: "Bonjour, comment ça va?" The classifier
# function returns the classification result, which is then printed.

from transformers import pipeline

classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
result = classifier("Bonjour, comment ça va?")
print(result)
# Output
# config.json: 
#  1.42k/? [00:00<00:00, 123kB/s]
# model.safetensors: 100%
#  1.11G/1.11G [00:38<00:00, 23.4MB/s]
# tokenizer_config.json: 100%
#  502/502 [00:00<00:00, 64.2kB/s]
# sentencepiece.bpe.model: 100%
#  5.07M/5.07M [00:00<00:00, 8.30MB/s]
# tokenizer.json: 
#  9.08M/? [00:00<00:00, 125MB/s]
# special_tokens_map.json: 100%
#  239/239 [00:00<00:00, 30.5kB/s]
# [{'label': 'fr', 'score': 0.9934879541397095}]


# Example 3: Text generation using pipeline()
# In this example, you will use the pipeline() function to perform text generation. You will load a pretrained text
# generation model and use it to generate text based on a given prompt.
# Initialize the text generation model:
# We initialize the pipeline for the text-generation task, specifying the model "gpt2". GPT-2 is a
# well-known model for text generation tasks.

# Initialize the text generation pipeline with GPT-2
generator = pipeline("text-generation", model="gpt2")

# Generate text based on a given prompt:
# We use the generator to generate text based on a prompt: "Once upon a time". Let's specify max_length=50,
# truncation=True to limit the generated text to 50 tokens and num_return_sequences=1 to generate one sequence.
# The generator function returns the generated text, which is then printed.

# Generate text based on a given prompt
prompt = "Once upon a time"
result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)

# Print the generated text
print(result[0]['generated_text'])
# Output
# Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.


# Example 4: Text generation using T5 with pipeline()
# In this example, you will use the pipeline() function to perform text-to-text generation with the T5 model.
# You will load a pretrained T5 model and use it to translate a sentence from English to French based on a given prompt.
# Initialize the text generation model:
# We initialize the pipeline for the `text2text-generation task, specifying the model "t5-small".
# T5 is a versatile model that can perform various text-to-text generation tasks, including translation.

# Initialize the text generation pipeline with T5
generator = pipeline("text2text-generation", model="t5-small")
# config.json: 100%
#  1.21k/1.21k [00:00<00:00, 166kB/s]
# model.safetensors: 100%
#  242M/242M [00:02<00:00, 122MB/s]
# generation_config.json: 100%
#  147/147 [00:00<00:00, 18.4kB/s]
# tokenizer_config.json: 100%
#  2.32k/2.32k [00:00<00:00, 385kB/s]
# spiece.model: 100%
#  792k/792k [00:00<00:00, 31.5MB/s]
# tokenizer.json: 100%
#  1.39M/1.39M [00:00<00:00, 46.0MB/s]

# Generate text based on a given prompt:
# We use the generator to translate a sentence from English to French based on the prompt:
# "translate English to French: How are you?". Let's specify max_length=50 to limit the generated text to
# 50 tokens and num_return_sequences=1 to generate one sequence. The generator function returns
# the translated text, which is then printed.

# Generate text based on a given prompt
prompt = "translate English to French: How are you?"
result = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(result[0]['generated_text'])
# Output
# Comment êtes-vous?

# Fill-mask task using BERT with pipeline()
# In this exercise, you will use the pipeline() function to perform a fill-mask task using the BERT model.
# load a pretrained BERT model and use it to predict the masked word in a given sentence.
# Instructions
# 1.	Initialize the fill-mask pipeline with the BERT model.
# 2.	Create a prompt with a masked token.
# 3.	Generate text by filling in the masked token.
# 4.	Print the generated text with the predictions.
#
# Initialize the fill-mask pipeline with BERT
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Generate text by filling in the masked token
prompt = "The capital of France is [MASK]."
result = fill_mask(prompt)

# Print the generated text
print(result)