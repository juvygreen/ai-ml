# # All Libraries required for this lab are listed below.
# # The libraries pre-installed on Skills Network Labs are commented.
# # Uncomment these if you are running the lab in your localenvironment, to install these packages.
#
# # !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1
# # - Update a specific package
# # !pip install pmdarima -U
# # - Update a package to specific version
# # !pip install --upgrade pmdarima==2.0.2
# # Note: If your environment doesn't support "!pip install", use "!mamba install"
#
# The following required libraries are not pre-installed in the Skills Network Labs environment. You will need to run the following cell to install them:
# !pip install dash==2.9.3 dash-core-components==2.0.0 dash-html-components==2.0.0 dash-table==5.0.0
# !pip install -U portalocker>=2.0.0
# !pip install torchtext torchdata
# !pip install plotly

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import math

import torch
import torch.nn as nn

from sklearn.manifold import TSNE

from torch.utils.data import DataLoader
import numpy as np
from torchtext.datasets import AG_NEWS
from IPython.display import Markdown as md
from tqdm import tqdm

from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torchtext.datasets import AG_NEWS
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
from sklearn.manifold import TSNE
import plotly.graph_objs as go
import pickle

from torch.nn.utils.rnn import pad_sequence
#Defining helper functions
def plot(COST,ACC):

    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()  # otherwise the right y-label is slightly clipped

    plt.show()
def plot_embdings(my_embdings,name,vocab):

  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')

  # Plot the data points
  ax.scatter(my_embdings[:,0], my_embdings[:,1], my_embdings[:,2])

  # Label the points
  for j, label in enumerate(name):
      i=vocab.get_stoi()[label]
      ax.text(my_embdings[j,0], my_embdings[j,1], my_embdings[j,2], label)

  # Set axis labels
  ax.set_xlabel('X Label')
  ax.set_ylabel('Y Label')
  ax.set_zlabel('Z Label')

  # Show the plot
  plt.show()



def plot_tras(words, model):
    # Tokenize the input words using a tokenizer function
    tokens = tokenizer(words)

    # Define the model's embedding dimension (d_model)
    d_model = 100

    # Convert the input words to a PyTorch tensor and move it to the specified device
    x = torch.tensor(text_pipeline(words)).unsqueeze(0).to(device)

    # Apply the model's embedding layer and scale the embeddings by sqrt(d_model)
    x_ = model.emb(x) * math.sqrt(d_model)

    # Apply the model's positional encoder to the embeddings
    x = model.pos_encoder(x_)

    # Extract projection weights for query, key, and value from the model's state_dict
    q_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][0:embed_dim].t()
    k_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()
    v_proj_weight = model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()

    # Calculate query (Q), key (K), and value (V) matrices
    Q = (x @ q_proj_weight).squeeze(0)
    K = (x @ k_proj_weight).squeeze(0)
    V = (x @ v_proj_weight).squeeze(0)

    # Calculate attention scores using dot-product attention
    scores = Q @ K.T

    # Set row and column labels for the attention matrix
    row_labels = tokens
    col_labels = row_labels

    # Create a heatmap of the attention scores
    plt.figure(figsize=(10, 8))
    plt.imshow(scores.cpu().detach().numpy())
    plt.yticks(range(len(row_labels)), row_labels)
    plt.xticks(range(len(col_labels)), col_labels, rotation=90)
    plt.title("Dot-Product Attention")
    plt.show()

    # Apply softmax to the attention scores and create a heatmap
    att = nn.Softmax(dim=1)(scores)
    plt.figure(figsize=(10, 8))
    plt.imshow(att.cpu().detach().numpy())
    plt.yticks(range(len(row_labels)), row_labels)
    plt.xticks(range(len(col_labels)), col_labels, rotation=90)
    plt.title("Scaled Dot-Product Attention")
    plt.show()

    # Calculate the attention head by multiplying softmax scores with values (V)
    head = nn.Softmax(dim=1)(scores) @ V

    # Visualize the embeddings and attention heads using t-SNE
    tsne(x_, tokens, title="Embeddings")
    tsne(head, tokens, title="Attention Heads")


def tsne(embeddings, tokens, title="Embeddings"):
    # Initialize t-SNE with 2 components and a fixed random state
    tsne = TSNE(n_components=2, random_state=0)

    # Fit t-SNE to the embeddings (converting from GPU if necessary)
    tsne_result = tsne.fit_transform(embeddings.squeeze(0).cpu().detach().numpy())

    # Create a scatter plot of the t-SNE results
    plt.scatter(tsne_result[:, 0], tsne_result[:, 1])

    # Set a title for the plot
    plt.title(title)

    # Add labels for each point in the scatter plot
    for j, label in enumerate(tokens):
        # Place the label text at the corresponding t-SNE coordinates
        plt.text(tsne_result[j, 0], tsne_result[j, 1], label)

    plt.show()

def save_list_to_file(lst, filename):
    """
    Save a list to a file using pickle serialization.

    Parameters:
        lst (list): The list to be saved.
        filename (str): The name of the file to save the list to.

    Returns:
        None
    """
    with open(filename, 'wb') as file:
        pickle.dump(lst, file)

def load_list_from_file(filename):
    """
    Load a list from a file using pickle deserialization.

    Parameters:
        filename (str): The name of the file to load the list from.

    Returns:
        list: The loaded list.
    """
    with open(filename, 'rb') as file:
        loaded_list = pickle.load(file)
    return loaded_list

#Toy dataset
#These are essentially the same steps that you have done in the previous labs. However, let's have a brief explanation.
# The code defines a dataset, tokenizes the text data using a basic English tokenizer, creates a vocabulary from
# the tokenized data, and sets up a default index for handling unknown tokens.
dataset = [
    (1,"Introduction to NLP"),
    (2,"Basics of PyTorch"),
    (1,"NLP Techniques for Text Classification"),
    (3,"Named Entity Recognition with PyTorch"),
    (3,"Sentiment Analysis using PyTorch"),
    (3,"Machine Translation with PyTorch"),
    (1," NLP Named Entity,Sentiment Analysis,Machine Translation "),
    (1," Machine Translation with NLP "),
    (1," Named Entity vs Sentiment Analysis  NLP "),
    (3,"he painted the car red"),
    (1,"he painted the red car")
    ]

tokenizer = get_tokenizer("basic_english")

def yield_tokens(data_iter):
    for  _,text in data_iter:
        yield tokenizer(text)

vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=["<unk>"])
vocab.set_default_index(vocab["<unk>"])

#Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to
# process the raw data strings from the dataset iterators.
#The function text_pipeline will tokenize the input text, and vocab will then be applied to get the token indices.
# The label_pipeline will ensure that the labels start at zero.
#These pipelines are defined here for future use.
def text_pipeline(x):
  return vocab(tokenizer(x))

def label_pipeline(x):
   return int(x) ‚Äì 1

#Zero padding
#In numerous PyTorch applications, ensuring consistent batch sizes is a fundamental requirement.
# This necessitates the use of zero-padding to harmonize varying sequence lengths within each batch,
# you can do this using pad_sequence, consider the list of tensors of different lengths:
sequences = [torch.tensor([j for j in range(1,i)]) for i in range(2,10)]
sequences

# [tensor([1]),
#  tensor([1, 2]),
#  tensor([1, 2, 3]),
#  tensor([1, 2, 3, 4]),
#  tensor([1, 2, 3, 4, 5]),
#  tensor([1, 2, 3, 4, 5, 6]),
#  tensor([1, 2, 3, 4, 5, 6, 7]),
#  tensor([1, 2, 3, 4, 5, 6, 7, 8])]


padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)
print(padded_sequences)
tensor([[1, 0, 0, 0, 0, 0, 0, 0],
        [1, 2, 0, 0, 0, 0, 0, 0],
        [1, 2, 3, 0, 0, 0, 0, 0],
        [1, 2, 3, 4, 0, 0, 0, 0],
        [1, 2, 3, 4, 5, 0, 0, 0],
        [1, 2, 3, 4, 5, 6, 0, 0],
        [1, 2, 3, 4, 5, 6, 7, 0],
        [1, 2, 3, 4, 5, 6, 7, 8]])

# Positional encodings
#Positional encodings play a pivotal role in transformers and various sequence-to-sequence models, aiding in
# conveying critical information regarding the positions or sequencing of elements within a given sequence.
# To illustrate, let's examine the sentences: "He painted the car red" and "He painted the red car."
# Despite their distinct meanings, it's worth noting that the embeddings for these sentences remain
# identical as shown here:
my_tokens='he painted the car red he painted the red car'

my_index=text_pipeline(my_tokens)
my_index

embedding_dim=3

vocab_size=len(vocab)
print(vocab_size)

embedding = nn.Embedding(vocab_size, embedding_dim)
#27

my_embdings=embedding(torch.tensor(my_index)).detach().numpy()
plot_embdings(my_embdings,tokenizer(my_tokens),vocab)

position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)
position
# tensor([[ 0.],
#         [ 1.],
#         [ 2.],
#         [ 3.],
#         [ 4.],
#         [ 5.],
#         [ 6.],
#         [ 7.],
#         [ 8.],
#         [ 9.],
#         [10.],
#         [11.],
#         [12.],
#         [13.],
#         [14.],
#         [15.],
#         [16.],
#         [17.],
#         [18.],
#         [19.],
#         [20.],
#         [21.],
#         [22.],
#         [23.],
#         [24.],
#         [25.],
#         [26.]])

d_model=3
pe = torch.zeros(vocab_size,d_model )
pe=torch.cat((position, position, position), 1)
pe
# tensor([[ 0.,  0.,  0.],
#         [ 1.,  1.,  1.],
#         [ 2.,  2.,  2.],
#         [ 3.,  3.,  3.],
#         [ 4.,  4.,  4.],
#         [ 5.,  5.,  5.],
#         [ 6.,  6.,  6.],
#         [ 7.,  7.,  7.],
#         [ 8.,  8.,  8.],
#         [ 9.,  9.,  9.],
#         [10., 10., 10.],
#         [11., 11., 11.],
#         [12., 12., 12.],
#         [13., 13., 13.],
#         [14., 14., 14.],
#         [15., 15., 15.],
#         [16., 16., 16.],
#         [17., 17., 17.],
#         [18., 18., 18.],
#         [19., 19., 19.],
#         [20., 20., 20.],
#         [21., 21., 21.],
#         [22., 22., 22.],
#         [23., 23., 23.],
#         [24., 24., 24.],
#         [25., 25., 25.],
#         [26., 26., 26.]])
#
# only need positional encodings for each sequence of embeddings, and to determine this, simply count
# the number of embeddings in the sequence.
samples,dim=my_embdings.shape
samples,dim
#(10, 3)

#Once add the positional encodings to the embeddings and plot the results; you will observe that they are different.
pos_embding=my_embdings+pe[0:samples,:].numpy()
plot_embdings(pos_embding,tokenizer(my_tokens),vocab)

pos_embding[3]# add -3 to get original embedding
#array([1.2508035, 1.5755361, 2.6317477], dtype=float32)

pos_embding[-1]#add -9 original embedding
#array([7.2508035, 7.5755363, 8.631748 ], dtype=float32)

pe=torch.cat((0.1*position, -0.1*position, 0*position), 1)
#plot the positional encodings.
plt.plot(pe[:, 0].numpy(), label="Dimension 1")
plt.plot(pe[:, 1].numpy(), label="Dimension 2")
plt.plot(pe[:, 2].numpy(), label="Dimension 3")

plt.xlabel("Sequence Number")
plt.legend()
plt.show()


pos_embding=my_embdings+pe[0:samples,:].numpy()
plot_embdings(pos_embding,tokenizer(my_tokens),vocab)

# Periodic functions, such as sine and cosine functions, possess the property of periodicity.
# This means they repeat their values over a regular interval, preventing them from growing too rapidly.
pe=torch.cat((torch.sin(2*3.14*position/6), 0*position+1, 0*position+1), 1)
pos_embding=my_embdings+pe[0:samples,:].numpy()
plot_embdings(pos_embding,tokenizer(my_tokens),vocab)


pe
# tensor([[ 0.0000,  1.0000,  1.0000],
#         [ 0.8658,  1.0000,  1.0000],
#         [ 0.8666,  1.0000,  1.0000],
#         [ 0.0016,  1.0000,  1.0000],
#         [-0.8650,  1.0000,  1.0000],
#         [-0.8673,  1.0000,  1.0000],
#         [-0.0032,  1.0000,  1.0000],
#         [ 0.8642,  1.0000,  1.0000],
#         [ 0.8681,  1.0000,  1.0000],
#         [ 0.0048,  1.0000,  1.0000],
#         [-0.8634,  1.0000,  1.0000],
#         [-0.8689,  1.0000,  1.0000],
#         [-0.0064,  1.0000,  1.0000],
#         [ 0.8626,  1.0000,  1.0000],
#         [ 0.8697,  1.0000,  1.0000],
#         [ 0.0080,  1.0000,  1.0000],
#         [-0.8617,  1.0000,  1.0000],
#         [-0.8705,  1.0000,  1.0000],
#         [-0.0096,  1.0000,  1.0000],
#         [ 0.8609,  1.0000,  1.0000],
#         [ 0.8713,  1.0000,  1.0000],
#         [ 0.0111,  1.0000,  1.0000],
#         [-0.8601,  1.0000,  1.0000],
#         [-0.8721,  1.0000,  1.0000],
#         [-0.0127,  1.0000,  1.0000],
#         [ 0.8593,  1.0000,  1.0000],
#         [ 0.8728,  1.0000,  1.0000]])
#
# You have observed an intriguing characteristic in the word embeddings: they are positioned closely enough to
# maintain proximity while remaining sufficiently distinct from each other. However, there is a notable exception
# when it comes to the embedding for "car." This anomaly arises because the sine wave used in positional
# encoding is inherently periodic, as illustrated in the image below, where you can observe the repetitive
# nature of the sine function. Consequently, the positional encoding for "car" at different locations
# within the sequence remains the same.
# # Plot the positional encodings with different line styles and markers
plt.plot(pe[:, 0].numpy(), label="Dimension 1", linestyle='-')
plt.plot(pe[:, 1].numpy(), label="Dimension 2", linestyle='--')
plt.plot(pe[:, 2].numpy(), label="Dimension 3", linestyle=':')

# Adjust the y-axis scale for better visibility
plt.ylim([-1, 1.1])

plt.xlabel("Sequence Number")
plt.legend()
plt.show()

pe = torch.cat((torch.cos(2 * 3.14 * position / 25), torch.sin(2 * 3.14 * position / 25), torch.sin(2 * 3.14 * position / 5)), 1)
pos_embding = my_embdings + pe[0:samples, :].numpy()
plot_embdings(pos_embding, tokenizer(my_tokens), vocab)

plt.plot(pe[:, 0].numpy(), label="Dimension 1")
plt.plot(pe[:, 1].numpy(), label="Dimension 2")
plt.plot(pe[:, 2].numpy(), label="Dimension 3")


# Overall, periodic functions provide a more expressive and adaptable means of encoding positional
# information in sequences, making them a preferred choice in modern sequence-to-sequence models like transformers.
# They offer improved capabilities for modeling both short-range and long-range dependencies,
# which are crucial for tasks in natural language processing and other domains. You can use the following class
# to apply Positional Encoding. The dropout probability is applied to the positional encodings to prevent overfitting.
from torch import nn

class PositionalEncoding(nn.Module):
    """
    https://pytorch.org/tutorials/beginner/transformer_tutorial.html
    """

    def __init__(self, d_model, vocab_size=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(vocab_size, d_model)
        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float()
            * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[:, : x.size(1), :]
        return self.dropout(x)


# Encoder layer
# What is self-attention?
# Each word (or token) in a sentence is compared to every other word, including itself,
# to determine how much attention or importance should be assigned to each word.
# Three set of vectors - query, key and value are created for the tokens.
# Let's have a look at an analogy which will paint a better picture for these vectors.
nn.TransformerEncoderLayer

my_embdings=embedding(torch.tensor(my_index))
my_embdings
# tensor([[ 0.6452, -0.1627,  1.6389],
#         [-0.1174,  0.6257, -0.7504],
#         [ 0.2803,  0.8602, -1.5533],
#         [-1.7492, -1.4245, -0.3683],
#         [ 0.0934, -1.1821,  1.1968],
#         [ 0.6452, -0.1627,  1.6389],
#         [-0.1174,  0.6257, -0.7504],
#         [ 0.2803,  0.8602, -1.5533],
#         [ 0.0934, -1.1821,  1.1968],
#         [-1.7492, -1.4245, -0.3683]], grad_fn=<EmbeddingBackward0>)
#
# There are ten embeddings, each with three dimensions.
my_embdings.shape
#torch.Size([10, 3])

# This line of code initializes a Transformer Encoder Layer in PyTorch using the nn.TransformerEncoderLayer class
encoder_layer=nn.TransformerEncoderLayer(
            d_model=3,
            nhead=1,
            dim_feedforward=1,
            dropout=0,
        )
#In the context of transformers, your objective is to train the model to take an input sequence and effectively
# generate another sequence as its output, a fundamental task that underlies a wide range of natural
# language processing and sequence-to-sequence tasks.
out=encoder_layer(my_embdings)
out
# tensor([[-0.7130, -0.7012,  1.4142],
#         [-0.7756,  1.4119, -0.6363],
#         [-0.5929,  1.4084, -0.8155],
#         [-1.1129, -0.1992,  1.3121],
#         [-0.6881, -0.7259,  1.4140],
#         [-0.7130, -0.7012,  1.4142],
#         [-0.7756,  1.4119, -0.6363],
#         [-0.5929,  1.4084, -0.8155],
#         [-0.6881, -0.7259,  1.4140],
#         [-1.1129, -0.1992,  1.3121]], grad_fn=<NativeLayerNormBackward0>)


out.mean(dim=1)
# tensor([ 0.0000e+00,  1.9868e-08,  0.0000e+00, -3.9736e-08,  7.9473e-08,
#          0.0000e+00,  1.9868e-08,  0.0000e+00,  7.9473e-08, -3.9736e-08],
#        grad_fn=<MeanBackward1>)

#access the parameters of the transformer, which encompass several crucial components.
# Among these, the key, query, and value parameters are particularly significant, playing pivotal roles in
# the model's attention mechanism and overall functionality.
params_dict = encoder_layer.state_dict()
# Print the parameter names and shapes
for name, param in params_dict.items():
    print(name, param.shape)
# self_attn.in_proj_weight torch.Size([9, 3])
# self_attn.in_proj_bias torch.Size([9])
# self_attn.out_proj.weight torch.Size([3, 3])
# self_attn.out_proj.bias torch.Size([3])
# linear1.weight torch.Size([1, 3])
# linear1.bias torch.Size([1])
# linear2.weight torch.Size([3, 1])
# linear2.bias torch.Size([3])
# norm1.weight torch.Size([3])
# norm1.bias torch.Size([3])
# norm2.weight torch.Size([3])
# norm2.bias torch.Size([3])
# The key, query, and value parameters are shown here.
embed_dim=3
q_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][0:embed_dim].t()
k_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][embed_dim:2*embed_dim].t()
v_proj_weight = encoder_layer.state_dict()['self_attn.in_proj_weight'][2*embed_dim:3*embed_dim].t()

# Here, ùëã represents the embeddings and ùëä represents learnable weights.
Q=my_embdings@q_proj_weight
K=my_embdings@k_proj_weight
V=my_embdings@v_proj_weight
#Attention scores: Scores=ùëÑùêæùëáùëëùëò‚àö
scores=Q@K.T/np. sqrt(embed_dim)
scores


#calculate the attention weights using the softmax function applied to the Scores and then multiply them by the values:
ùëÇ=softmax(Scores)‚ãÖùëâ
head=nn.Softmax(dim=1)(scores)@V
head


transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=2)

#display the other layer.
params_dict = transformer_encoder.state_dict()
for name, param in params_dict.items():
    print(name, param.shape)



# Text classification
# Let‚Äôs build a text classification model using PyTorch and torchtext to classify news articles into one of the four categories: World, Sports, Business, and Sci/Tech.
# Import bank dataset
# Load the AG_NEWS dataset for the train split and split it into input text and corresponding labels:
train_iter= AG_NEWS(split=‚Äùtrain‚Äù)
# The AG_NEWS dataset in torchtext does not support direct indexing like a list or tuple.
# It is not a random access dataset but rather an iterable dataset that needs to be used with an iterator.
# This approach is more effective for text data.
Y,text= next(iter(train_iter ))
print(y,text)
#3 Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\band of ultra-cynics, are seeing green again.

# You can find the label of the sample.
Ag_news_label = {1: ‚ÄúWorld‚Äù, 2: ‚ÄúSports‚Äù, 3: ‚ÄúBusiness‚Äù, 4: ‚ÄúSci/Tec‚Äù}
ag_news_label[y]
#'Business'

#can also use the dataset to find all the classes.
Num_class = len(set([label for (label, text) in train_iter ]))
num_class
#4

#can build the vocabulary as before, just using the AG dataset to obtain token indices
vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[‚Äú<unk>‚Äù])
vocab.set_default_index(vocab[‚Äú<unk>‚Äù])
#Here are some token indices:
vocab([‚Äúage‚Äù,‚Äùhello‚Äù])
#[2120, 12544]

#Dataset
#You can convert the dataset into map-style datasets and then perform a random split to create separate training
# and validation datasets. The training dataset will contain 95% of the samples, while the validation dataset will
# contain the remaining 5%. These datasets can be used for training and evaluating a machine learning model
# for text classification on the AG_NEWS dataset.
# Split the dataset into training and testing iterators.
train_iter, test_iter = AG_NEWS()

# Convert the training and testing iterators to map-style datasets.
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)

# Determine the number of samples to be used for training and validation (5% for validation).
num_train = int(len(train_dataset) * 0.95)

# Randomly split the training dataset into training and validation datasets using `random_split`.
# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

#The code checks if a CUDA-compatible GPU is available in the system using PyTorch, a popular deep learning framework.
# If a GPU is available, it assigns the device variable to "cuda" (which stands for CUDA, the parallel computing
# platform and application programming interface model developed by NVIDIA). If a GPU is not available,
# it assigns the device variable to "cpu" (which means the code will run on the CPU instead).
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
device(type='cpu')


#Data loader
#In PyTorch, the collate_fn function is used in conjunction with data loaders to customize the way batches are
# created from individual samples. The provided code defines a collate_batch function in PyTorch,
# which is used with data loaders to customize batch creation from individual samples.
# It processes a batch of data, including labels and text sequences.
# It applies the label_pipeline and text_pipeline functions to preprocess the labels and texts, respectively.
# The processed data is then converted into PyTorch tensors and returned as a tuple containing the label tensor,
# text tensor, and offsets tensor representing the starting positions of each text sequence in the combined tensor.
# The function also ensures that the returned tensors are moved to the specified device (e.g., GPU)
# for efficient computation.
from torch.nn.utils.rnn import pad_sequence

def collate_batch(batch):
    label_list, text_list = [], []
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))


    label_list = torch.tensor(label_list, dtype=torch.int64)
    text_list = pad_sequence(text_list, batch_first=True)


    return label_list.to(device), text_list.to(device)

#can convert the dataset objects to a data loader by applying the collate function.
BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)

#can see the output sequence when you have the label, text, and offsets for each batch.
label,seqence=next(iter(valid_dataloader ))


#Neural network
#You have created a neural network for a text classification model using an EmbeddingBag layer, followed by a
# softmax output layer. Additionally, you have initialized the model using a specific method.
class Net(nn.Module):
    """
    Text classifier based on a pytorch TransformerEncoder.
    """

    def __init__(

        self,
        vocab_size,
        num_class,
        embedding_dim=100,
        nhead=5,
        dim_feedforward=2048,
        num_layers=6,
        dropout=0.1,
        activation="relu",
        classifier_dropout=0.1):

        super().__init__()

        self.emb = nn.Embedding(vocab_size,embedding_dim)

        self.pos_encoder = PositionalEncoding(
            d_model=embedding_dim,
            dropout=dropout,
            vocab_size=vocab_size,
        )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
        )
        self.classifier = nn.Linear(embedding_dim, num_class)
        self.d_model = embedding_dim

    def forward(self, x):
        x = self.emb(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)
        x = self.classifier(x)

        return x
y,x=next(iter(train_dataloader))
x
# tensor([[  77, 4376, 1351,  ...,    0,    0,    0],
#         [3088,   12,    9,  ...,    0,    0,    0],
#         [ 637, 5285,    2,  ...,    0,    0,    0],
#         ...,
#         [1586,  874,  242,  ...,    0,    0,    0],
#         [  11,  185,    3,  ...,    0,    0,    0],
#         [2258, 1948, 3302,  ...,    0,    0,    0]])
#
# You have created the model, and the embedding dimension size is a free parameter.
emsize=64

#need the vocabulary size to determine the number of embeddings.
vocab_size=len(vocab)
vocab_size
#95811

#also determined the number of classes for the output layer.
num_class
#4
#Creating the model:
model = Net(vocab_size=vocab_size,num_class=4).to(device)
model


# The code line predicted_label=model(text, offsets) is used to obtain predicted labels from a machine learning
# model for a given input text and its corresponding offsets. The model is the machine learning model being
# used for text classification or similar tasks.
predicted_label=model(x)

#can verify the output shape of your model. In this case, the model is trained with a mini-batch size of 64 samples.
# The output layer of the model produces 4 logits for each neuron, corresponding to the four classes in the
# classification task. You can also create a function to find the accuracy given a dataset.
predicted_label.shape
#torch.Size([64, 4])

x.shape
#torch.Size([64, 84])

#Function predict takes in a text and a text pipeline, which preprocesses the text for machine learning.
# It uses a pre-trained model to predict the label of the text for text classification on the AG_NEWS dataset.
# The function returns the predicted label as a result.
def predict(text, text_pipeline):
    with torch.no_grad():
        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)

        output = model(text)
        return ag_news_label[output.argmax(1).item() + 1]
predict("I like sports and stuff",text_pipeline )
#'Business'

#You can create a function to evaluate the model's accuracy on a dataset.
def evaluate(dataloader, model_eval):
    model_eval.eval()
    total_acc, total_count= 0, 0

    with torch.no_grad():
        for idx, (label, text) in enumerate(dataloader):
            predicted_label = model_eval(text.to(device))

            total_acc += (predicted_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count

#proceed to evaluate the model, and upon observation, found that its performance is no better than average.
# This outcome is expected, considering that the model has not undergone any training yet.
evaluate(test_dataloader, model)


# Training
# set the learning rate (LR) to 0.1, which determines the step size at which the optimizer updates the model's
# parameters during training. The CrossEntropyLoss criterion is used to calculate the loss between the model's
# predicted outputs and the ground truth labels. This loss function is commonly employed for multi-class
# classification tasks.

# LR=0.1

# criterion = torch.nn.CrossEntropyLoss()
# optimizer = torch.optim.SGD(model.parameters(), lr=LR)
# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)
# Training the model for 10 epochs.
# skip this step if you don't have GPU. Retrieve and use the model has been trained for 100 epochs and saved, in the next step.
# EPOCHS = 10
# cum_loss_list=[]
# acc_epoch=[]
# acc_old=0

# for epoch in tqdm(range(1, EPOCHS + 1)):
#     model.train()
#     cum_loss=0
#     for idx, (label, text) in enumerate(train_dataloader):
#         optimizer.zero_grad()
#         label, text=label.to(device), text.to(device)


#         predicted_label = model(text)
#         loss = criterion(predicted_label, label)
#         loss.backward()
#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
#         optimizer.step()
#         cum_loss+=loss.item()
#     print("Loss",cum_loss)

#     cum_loss_list.append(cum_loss)
#     accu_val = evaluate(valid_dataloader)
#     acc_epoch.append(accu_val)

#     if accu_val > acc_old:
#       acc_old= accu_val
#       torch.save(model.state_dict(), 'my_model.pth')

# save_list_to_file(lst=cum_loss_list, filename="loss.pkl")
# save_list_to_file(lst=acc_epoch, filename="acc.pkl")

#have the capability to upload the trained model along with comprehensive data on cumulative loss and average
# accuracy at each epoch.
! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05RNEN/my_model.pth
! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05RNEN/acc.pkl
! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX05RNEN/loss.pkl
cum_loss_list=load_list_from_file("loss.pkl")
acc_epoch=load_list_from_file("acc.pkl")
pretrained_model_path = 'my_model.pth'
model_ = Net(vocab_size=vocab_size, num_class=4).to(device)
model_.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device('cpu')))
plot(cum_loss_list,acc_epoch)

#can evaluate the results on the test data; you achieve over 80%.
evaluate(test_dataloader, model_)
sample=15

words=train_dataset[sample][1]
print(words)
plot_tras(words, model)
#In scaled dot-product attention, if you look from x-axis, rescuing is important with words
# like soon, don, their, help and an.
sample=1

words=train_dataset[sample][1]
print(words)
plot_tras(words, model)

# make a prediction on the following article using the function predict.
article="""Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming 
from behind to claim a vital 2-1 victory at the Women‚Äôs World Cup.
Katie McCabe opened the scoring with an incredible Olimpico goal ‚Äì scoring straight from a corner kick ‚Äì as 
her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular 
Stadium in Australia.
Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to 
get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.
Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home 
from the edge of the area to seal the three points."""

#This markdown content generates a styled box with light gray background and padding.
# It contains an <h3> header displaying the content of the article variable, and an <h4> header
# indicating the predicted category of the news article which is provided by the result variable.
# The placeholders {article} and {result} will be dynamically replaced with actual values
# when this markdown is rendered.
result = predict(article, text_pipeline)

markdown_content = f'''
<div style="background-color: gray; padding: 10px;">
    <h3>{article}</h3>
    <h4>The category of the news article: {result}</h4>
</div>
'''

md(markdown_content)
