# !pip install -U torchdata==0.5.1
# !pip install -U spacy==3.7.2
# !pip install -Uqq portalocker==2.7.0
# !pip install -qq torchtext==0.14.1
# !pip install -Uq nltk==3.8.1
#
# !python -m spacy download de
# !python -m spacy download en
#
# !pip install pdfplumber==0.9.0
# !pip install fpdf==1.7.2
#
# !wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'
# !wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'
# !wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'
# --2025-09-25 18:09:32--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py
# Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
# Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 4890 (4.8K) [application/x-python]
# Saving to: ‘Multi30K_de_en_dataloader.py’
#
# Multi30K_de_en_data 100%[===================>]   4.78K  --.-KB/s    in 0s
#
# 2025-09-25 18:09:32 (41.2 MB/s) - ‘Multi30K_de_en_dataloader.py’ saved [4890/4890]
#
# --2025-09-25 18:09:33--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt
# Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
# Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 144559760 (138M) [binary/octet-stream]
# Saving to: ‘transformer.pt’
#
# transformer.pt      100%[===================>] 137.86M  39.3MB/s    in 3.6s
#
# 2025-09-25 18:09:38 (38.4 MB/s) - ‘transformer.pt’ saved [144559760/144559760]
#
# --2025-09-25 18:09:40--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf
# Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104, 169.63.118.104
# Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 27628 (27K) [application/pdf]
# Saving to: ‘input_de.pdf’
#
# input_de.pdf        100%[===================>]  26.98K  --.-KB/s    in 0.001s
#
# 2025-09-25 18:09:40 (30.3 MB/s) - ‘input_de.pdf’ saved [27628/27628]
#
# Importing required libraries
from torchtext.datasets import multi30k, Multi30k
import torch
from typing import Iterable, List
import matplotlib.pyplot as plt
from nltk.translate.bleu_score import sentence_bleu
from torch import Tensor
import torch
import torch.nn as nn
from torch.nn import Transformer
import math
from tqdm import tqdm

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
# Define special symbols and indices
UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3
# Make sure the tokens are in order of their indices to properly insert them in vocab
special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']


# DataLoader
# In the English-German Multi30K dataset, you first load the data and break down sentences into words or smaller
# pieces, called tokens. From these tokens, you create a unique list or vocabulary. Each token is then turned into
# a specific number using this vocabulary. Because sentences can be of different lengths, add padding to make them
# all the same size in a batch. All this processed data is then organized into a PyTorch DataLoader,
# making it easy to use for training neural networks. A function has been provided for you to handle all these.
# %run Multi30K_de_en_dataloader.py
# You've set up data loaders for training and testing. Given the exploratory work, use a batch size of one
train_dataloader, _ = get_translation_dataloaders(batch_size = 1)

#Initialize an iterator for the validation data loader:
data_itr=iter(train_dataloader)
data_itr
#<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7b7a8ad22710>

#To obtain diverse examples, you can cycle through multiple samples since the dataset is sorted by length.
for n in range(1000):
    german, english= next(data_itr)

#The dataset is structured as sequence-batch-feature, rather than the typical batch-feature-sequence.
# For compatibility with your utility functions, you can transpose the dataset.
german=german.T
english=english.T

#print out the text by converting the indexes to words using index_to_german and index_to_english
for n in range(10):
    german, english= next(data_itr)

    print("sample {}".format(n))
    print("german input")
    print(index_to_german(german))
    print("english target")
    print(index_to_eng(english))
    print("_________\n")

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
DEVICE
#device(type='cpu')


# Important concepts
# Masking
# During training, the entire sequence is visible to the model and used as input to learn patterns. In contrast,
# for prediction, the future sequence is not available. To do this, employ masking to simulate this lack of future
# data, ensuring the model learns to predict without seeing the actual next tokens. It is crucial for ensuring
# certain positions are not attended to. The function generate_square_subsequent_mask produces an upper
# triangular matrix, which ensures that during decoding, a token can't attend to future tokens.
def generate_square_subsequent_mask(sz,device=DEVICE):
    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask

#The create_mask function, on the other hand, generates both source and target masks, as well as padding masks
# based on the provided source and target sequences. The padding masks ensure that the model doesn't attend to
# pad tokens, providing a streamlined attention.
def create_mask(src, tgt,device=DEVICE):
    src_seq_len = src.shape[0]
    tgt_seq_len = tgt.shape[0]

    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)
    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)

    src_padding_mask = (src == PAD_IDX).transpose(0, 1)
    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)
    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask

#Positional encoding
#The transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model
# this information, positional encodings are added to the tokens embeddings. These encodings have a fixed
# pattern based on their position in the sequence.
# Add positional information to the input tokens
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding[:, 0::2] = torch.sin(pos * den)
        pos_embedding[:, 1::2] = torch.cos(pos * den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

#Token embedding
#Token embedding, also known as word embedding or word representation, is a way to convert words or tokens
# from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the
# corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties
# of the word, such as its meaning, context, or relationships with other words.
#The TokenEmbedding class below converts numerical tokens into embeddings:
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)


# the Seq2SeqTransformer class, which represents the core of the transformer model for language translation.
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 src_vocab_size: int,
                 tgt_vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1):
        super(Seq2SeqTransformer, self).__init__()

        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout)
        self.generator = nn.Linear(emb_size, tgt_vocab_size)

    def forward(self,
                src: Tensor,
                trg: Tensor,
                src_mask: Tensor,
                tgt_mask: Tensor,
                src_padding_mask: Tensor,
                tgt_padding_mask: Tensor,
                memory_key_padding_mask: Tensor):
        src_emb = self.positional_encoding(self.src_tok_emb(src))
        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
        outs =outs.to(DEVICE)
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor):
        return self.transformer.encoder(self.positional_encoding(
                            self.src_tok_emb(src)), src_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):
        return self.transformer.decoder(self.positional_encoding(
                          self.tgt_tok_emb(tgt)), memory,
                          tgt_mask)


# Inference
Torch.manual_seed(0)

SRC_LANGUAGE = ‘de’
TGT_LANGUAGE = ‘en’
SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])
TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 128
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3

transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)

for p in transformer.parameters():
    if p.dim() > 1:
        nn.init.xavier_uniform_(p)

transformer = transformer.to(DEVICE)

#start off with a trained model.For this, load the weights of the transformer model from the file ‘transformer.pt’.
transformer.load_state_dict(torch.load(‘transformer.pt’, map_location=DEVICE, ))
#<All keys matched successfully>

#Since the dataset is organized by sequence length, let’s iterate through it to obtain a longer sequence
for n in range(100):
    src ,tgt= next(data_itr)

#Display the source sequence in German that you aim to translate, alongside the target sequence
# in English that you want your model to produce
print(“ nglish target”,index_to_eng(tgt))
print(“german input”,index_to_german(src))
#engish target <bos> People walking around a downtown area of a city . <eos>
#german input <bos> Menschen gehen in einem Innenstadtbereich entlang . <eos>

#find the number of tokens in the German sample:
num_tokens = src.shape[0]
num_tokens
#9

#construct a mask to delineate which inputs are factored into the attention computation.
# Given that this pertains to a translation task, all tokens in the source sequence are accessible,
# thus setting the mask values to false.
src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )
src_mask[0:10]
# tensor([[False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False],
#         [False, False, False, False, False, False, False, False, False]])
#
# Extract the first sample from the batch of sequences slated for translation. While currently redundant,
# this procedure will become relevant later as you handle larger batches.
Src_=src[:,0].unsqueeze(1)
print(src_.shape)
print(src.shape)
#torch.Size([9, 1])
#torch.Size([9, 1])


# The memory, which is the encoder’s output, encapsulates the original sequence to be translated and serves
# as the input for the decoder.
memory = transformer.encode(src_, src_mask)
memory.shape
#torch.Size([9, 1, 512])

#To indicate the beginning of an output sequence generation, initiate it with the start symbol:
ys = torch.ones(1, 1).fill_(BOS_IDX).type(torch.long).to(DEVICE)
ys
#tensor([[2]])

#Due to some naming conventions, the term “target” is used to denote the prediction.
# In this context, the “target” refers to the words following the current prediction.
# These can be combined with the source to make further predictions. Consequently,
# construct a target mask set to ‘false’ indicating that no values should be ignored:
tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)
tgt_mask
#tensor([[False]])

#feed the encoder’s output (referred to as ‘memory’) and the previous prediction from the transformer,
# which at this point is solely the start token, into the decoder:
out = transformer.decode(ys, memory, tgt_mask)
out.shape
#torch.Size([1, 1, 512])

#The decoder’s output is an enhanced word embedding representing the anticipated translation.
# At this point, the batch dimension is omitted.
Out = out.transpose(0, 1)
out.shape
#torch.Size([1, 1, 512])

#Once the decoder produces its output, it’s passed through output layer logit value over the vocabulary
# of 10837 words. Later on you will only need the last token so you can inputout[:, -1]
logit = transformer.generator(out[:, -1])
logit.shape
#torch.Size([1, 10837])

#The predicted word is determined by identifying the highest logit value, which signifies the model’s most probable
# translation for the input at a specific position; this position corresponds to the index of the next token.
_, next_word_index = torch.max(logit, dim=1)
print(“ nglish output:”,index_to_eng(next_word_index))
#engish output: People

#only need the integer for the index:
next_word_index=next_word_index.item()
next_word_index
#83

#Now, append the newly predicted word to the prior predictions, allowing the model to consider the entire sequence
# of generated words when making its next prediction.
ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)
ys


#To predict the subsequent word in the translation, update target mask and use the transformer decoder to derive
# the word probabilities. The word with the maximum probability is then selected as the prediction.
# Note that the encoder output contains all the information you need.
# Update the target mask for the current sequence length.
Tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)
tgt_mask
#tensor([[False,  True],
#        [False, False]])

# Decode the current sequence using the transformer and retrieve the output.
Out = transformer.decode(ys, memory, tgt_mask)
out = out.transpose(0, 1)
out.shape
#torch.Size([1, 2, 512])

out[:, -1].shape
#torch.Size([1, 512])

# Get the word probabilities for the last predicted word.
Prob = transformer.generator(out[:, -1])
# Find the word index with the highest probability.
_, next_word_index = torch.max(prob, dim=1)
# Print the predicted English word.
print(“English output:”, index_to_eng(next_word_index))
# Convert the tensor value to a Python scalar.
Next_word_index = next_word_index.item()
#English output: are

#Now, update the prediction.
Ys = torch.cat([ys,torch.ones(1, 1).type_as(src.data).fill_(next_word_index)], dim=0)
print(“ nglish output”,index_to_eng(ys))
#engish output <bos> People are



def greedy_decode(model, src, src_mask, max_len, start_symbol):
    src = src.to(DEVICE)
    src_mask = src_mask.to(DEVICE)

    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)
    for I in range(max_len-1):
        memory = memory.to(DEVICE)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                    .type(torch.bool)).to(DEVICE)
        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        if next_word == EOS_IDX:
            break
    return ys

#Retrieve the indices for the German language and generate the corresponding mask:
src
src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).to(DEVICE )

#Set a reasonable value for the max length of target sequence:
max_len=src.shape[0]+5
max_len
#14

#Apply the function greedy_decode to data:
ys=greedy_decode(transformer, src, src_mask, max_len, start_symbol=BOS_IDX)
print(“ nglish “,index_to_eng(ys))
#engish  <bos> People are walking down a city area . <eos>

#Notice that it works, but it’s not exactly the same. However, it’s still pretty good.
print(“ nglish “,index_to_eng(tgt))
#engish  <bos> People walking around a downtown area of a city . <eos>


# Decoding the differences: Training vs. inference in neural machine translation
with index PAD_IDX an input.
from torch.nn import CrossEntropyLoss

loss_fn = CrossEntropyLoss(ignore_index=PAD_IDX)

#Drop the last sample of the target
tgt_input = tgt[:-1, :]
print(index_to_eng(tgt_input))
print(index_to_eng(tgt))
#<bos> People walking around a downtown area of a city .
#<bos> People walking around a downtown area of a city . <eos>

#Create the required masks
src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
print(f"Shape of src_mask: {src_mask.shape}")
print(f"Shape of tgt_mask: {tgt_mask.shape}")
print(f"Shape of src_padding_mask: {src_padding_mask.shape}")
print(f"Shape of tgt_padding_mask: {tgt_padding_mask.shape}")
# Shape of src_mask: torch.Size([9, 9])
# Shape of tgt_mask: torch.Size([11, 11])
# Shape of src_padding_mask: torch.Size([1, 9])
# Shape of tgt_padding_mask: torch.Size([1, 11])

src_padding_mask
#tensor([[False, False, False, False, False, False, False, False, False]])

#In the target mask, each subsequent column incrementally reveals more tokens by introducing negative infinity values,
# thereby unblocking them. You can display the target mask to visualize the progression or specifically identify
# which tokens are being masked at each step.
print(tgt_mask)
[index_to_eng( tgt_input[t==0])  for t in tgt_mask] #index_to_eng(tgt_input))
# tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
#         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
#         [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
#         [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
#         [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
#         [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
#         [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
#         [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
# ['<bos>',
#  '<bos> People',
#  '<bos> People walking',
#  '<bos> People walking around',
#  '<bos> People walking around a',
#  '<bos> People walking around a downtown',
#  '<bos> People walking around a downtown area',
#  '<bos> People walking around a downtown area of',
#  '<bos> People walking around a downtown area of a',
#  '<bos> People walking around a downtown area of a city',
#  '<bos> People walking around a downtown area of a city .']


#Loss
#delve into how you can calculate the loss, you have your src and tgt_input
logits = transformer(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

print("output shape",logits.shape)
print("target shape",tgt_input.shape)
print("source shape ",src.shape)
#output shape torch.Size([11, 1, 10837])
#target shape torch.Size([11, 1])
#source shape  torch.Size([9, 1])


#Ground Truth for Prediction is simply shifted right and is called tgt_out , you can print out tokens:
tgt_out = tgt[1:, :]
print(tgt_out.shape)
[index_to_eng(t)  for t in tgt_out]
# torch.Size([11, 1])
# ['People',
#  'walking',
#  'around',
#  'a',
#  'downtown',
#  'area',
#  'of',
#  'a',
#  'city',
#  '.',
#  '<eos>']
#
# The token indices represent the classes you aim to predict. By flattening the tensor, each index becomes a
# distinct sample, serving as the target for the cross-entropy loss.
tgt_out_flattened = tgt_out.reshape(-1)
print(tgt_out_flattened.shape)
tgt_out_flattened
torch.Size([11])
#tensor([  83,   42,   87,    4, 1653,  178,   13,    4,  108,    5,    3])


#In this autoregressive model, showcase the input target tokens after the application of the mask.
["input: {} target: {}".format(index_to_eng( tgt_input[m==0]),index_to_eng( t))  for m,t in zip(tgt_mask,tgt_out)]
# ['input: <bos> target: People',
#  'input: <bos> People target: walking',
#  'input: <bos> People walking target: around',
#  'input: <bos> People walking around target: a',
#  'input: <bos> People walking around a target: downtown',
#  'input: <bos> People walking around a downtown target: area',
#  'input: <bos> People walking around a downtown area target: of',
#  'input: <bos> People walking around a downtown area of target: a',
#  'input: <bos> People walking around a downtown area of a target: city',
#  'input: <bos> People walking around a downtown area of a city target: .',
#  'input: <bos> People walking around a downtown area of a city . target: <eos>']

#calculate the loss as the output from the transformer's decoder is provided as input to the cross-entropy loss functio
# n along with the target sequence values.
loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
print(loss)
#tensor(0.8654, grad_fn=<NllLossBackward0>)


# Under the hood of loss calculation (Optional)
#That's it for loss calculation, but if you are curious how the loss is calculated here is what happens under
# the hood of calculating the Cross Entropy loss. First, check the shape of tensors before and after the reshaping:
# logits.reshape(-1, logits.shape[-1]) reshapes the logits tensor to a 2D tensor with a shape of
# [sequence_length * batch_size, vocab_size]. This reshaping is done to align both the predicted logits and
# target outputs for the loss calculation.
print("logit's shape is:",logits.shape)
logits_flattened = logits.reshape(-1, logits.shape[-1])
print("logit_flat's shape is:",logits_flattened.shape)


# tgt_out.reshape(-1) reshapes the tgt_out tensor to a 1D tensor by flattening it along the sequence and batch dimensions. This is done to align it with the reshaped logits.
print("tgt_out's shape is:",tgt_out.shape)
tgt_out_flattened = tgt_out.reshape(-1)
print("tgt_out_flat's shape is:",tgt_out_flattened.shape)
# logit's shape is: torch.Size([11, 1, 10837])
# logit_flat's shape is: torch.Size([11, 10837])
# tgt_out's shape is: torch.Size([11, 1])
# tgt_out_flat's shape is: torch.Size([11])
#
# Inside the loss function, logits will transform into probabilities between [0,1] that sum up to 1:
# Applying the Cross-Entropy Loss Function
probs = torch.nn.functional.softmax(logits_flattened, dim=1)
probs[1].sum()
# tensor(1.0000, grad_fn=<SumBackward0>)
#
# check the probabilities for some random tokens:

for i in range (5):
    # using argmax, you can retrieve the index of the token that is predicted with the highest probaility
    print("Predicted token id:",probs[i].argmax().item(), "predicted probaility:",probs[i].max().item())
    # you can also check the actual token from the tgt_out_flat
    print("Actual token id:",tgt_out_flattened[i].item(), "predicted probaility:", probs[i,tgt_out_flattened[i]].item(),"\n")
# Predicted token id: 83 predicted probaility: 0.9634031057357788
# Actual token id: 83 predicted probaility: 0.9634031057357788
#
# Predicted token id: 17 predicted probaility: 0.5974355936050415
# Actual token id: 42 predicted probaility: 0.19530464708805084
#
# Predicted token id: 7 predicted probaility: 0.3400070369243622
# Actual token id: 87 predicted probaility: 0.31182861328125
#
# Predicted token id: 4 predicted probaility: 0.955619215965271
# Actual token id: 4 predicted probaility: 0.955619215965271
#
# Predicted token id: 178 predicted probaility: 0.34674543142318726
# Actual token id: 1653 predicted probaility: 0.08681467175483704



# proceed with calculating the difference between the actual token's probability (1) and the
# predicted probabilities for each token:
neg_log_likelihood = torch.nn.functional.nll_loss(probs, tgt_out_flattened)
# Step 3: Obtaining the Loss Value
loss = neg_log_likelihood

# Print the total loss value
print("Loss:", loss.item())
Loss: -0.6082930564880371

#Evaluate
#By following the aforementioned procedures, you can develop a function that is capable of making predictions and
# subsequently computing the corresponding loss on the validation data, you will use this function later on.
def evaluate(model):
    model.eval()
    losses = 0



    for src, tgt in val_dataloader:
        src = src.to(DEVICE)
        tgt = tgt.to(DEVICE)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        losses += loss.item()

    return losses / len(list(val_dataloader))


#Training the model
#Incorporating the previously outlined steps, proceed to train the model. Apart from these specific procedures,
# the overall training process conforms to the conventional methods employed in neural network training.
# Now, write a function to train the model
def train_epoch(model, optimizer, train_dataloader):
    model.train()
    losses = 0

    # Wrap train_dataloader with tqdm for progress logging
    train_iterator = tqdm(train_dataloader, desc="Training", leave=False)

    for src, tgt in train_iterator:
        src = src.to(DEVICE)
        tgt = tgt.to(DEVICE)

        tgt_input = tgt[:-1, :]

        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        src_mask = src_mask.to(DEVICE)
        tgt_mask = tgt_mask.to(DEVICE)
        src_padding_mask = src_padding_mask.to(DEVICE)
        tgt_padding_mask = tgt_padding_mask.to(DEVICE)

        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)
        logits = logits.to(DEVICE)

        optimizer.zero_grad()

        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()

        optimizer.step()
        losses += loss.item()

        # Update tqdm progress bar with the current loss
        train_iterator.set_postfix(loss=loss.item())

    return losses / len(list(train_dataloader))

#The configuration for the translation model includes a source and target vocabulary size determined by the dataset
# languages, an embedding size of 512, 8 attention heads, a hidden dimension for the feed-forward network of 512,
# and a batch size of 128. The model is structured with three layers each in both the encoder and the decoder.
torch.manual_seed(0)

SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])
TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])
EMB_SIZE = 512
NHEAD = 8
FFN_HID_DIM = 512
BATCH_SIZE = 128
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3

#Create a train loader with a batch size of 128.
train_dataloader, val_dataloader = get_translation_dataloaders(batch_size = BATCH_SIZE)

#Create a transformer model.
transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
transformer = transformer.to(DEVICE)

#Initialize the weights of the transformer model.
#Insulate the Adam optimizer.
optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

#Initialize the train loss and validation loss list.
TrainLoss=[]
ValLoss=[]

#Train the model for 10 epochs using the above functions.

#be aware that training the model using CPUs can be a time-consuming process.
from timeit import default_timer as timer
NUM_EPOCHS = 10

for epoch in range(1, NUM_EPOCHS+1):
    start_time = timer()
    train_loss = train_epoch(transformer, optimizer, train_dataloader)
    TrainLoss.append(train_loss)
    end_time = timer()
    val_loss = evaluate(transformer)
    ValLoss.append(val_loss)
    print((f"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, "f"Epoch time = {(end_time - start_time):.3f}s"))
torch.save(transformer.state_dict(), 'transformer_de_to_en_model.pt')

#Plot the loss for the training and validation data.
epochs = range(1, len(TrainLoss) + 1)

plt.figure(figsize=(10, 5))
plt.plot(epochs, TrainLoss, 'r', label='Training loss')
plt.plot(epochs,ValLoss, 'b', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

#Loading the saved model
#If you want to skip training and load the pretrained model that is provided, go ahead and uncomment the following cell:
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer_de_to_en_model.pt'
transformer.load_state_dict(torch.load('transformer_de_to_en_model.pt',map_location=torch.device('cpu')))

#Translation and evaluation
#Using the greedy_decode function that you defined earlier, you can create a translator function that generates
# English translation of an input German text.
# translate input sentence into target language
def translate(model: torch.nn.Module, src_sentence: str):
    model.eval()
    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)
    num_tokens = src.shape[0]
    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
    tgt_tokens = greedy_decode(
        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()
    return " ".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace("<bos>", "").replace("<eos>", "")

#Now, let's look into some sample translations:
for n in range(5):
    german, english= next(data_itr)

    print("German Sentence:",index_to_german(german).replace("<bos>", "").replace("<eos>", ""))
    print("English Translation:",index_to_eng(english).replace("<bos>", "").replace("<eos>", ""))
    print("Model Translation:",translate(transformer,index_to_german(german)))
    print("_________\n")


#Evaluation with BLEU score
#To evaluate the generated translations, a function calculate_bleu_score is introduced.
# It computes the BLEU score, a common metric for machine translation quality, by comparing the
# generated translation to reference translations. The BLEU score provides a quantitative measure of translation accuracy.
#The code also includes an example of calculating the BLEU score for a generated translation.
def calculate_bleu_score(generated_translation, reference_translations):
    # convert the generated translations and reference translations into the expected format for sentence_bleu
    references = [reference.split() for reference in reference_translations]
    hypothesis = generated_translation.split()

    # calculate the BLEU score
    bleu_score = sentence_bleu(references, hypothesis)

    return bleu_score

generated_translation = translate(transformer,"Ein brauner Hund spielt im Schnee .")

reference_translations = [
    "A brown dog is playing in the snow .",
    "A brown dog plays in the snow .",
    "A brown dog is frolicking in the snow .",
    "In the snow, a brown dog is playing ."

]

bleu_score = calculate_bleu_score(generated_translation, reference_translations)
print("BLEU Score:", bleu_score, "for",generated_translation)


#Translating a document
#implement a feature that translates a PDF in German to English. To achieve this, you will leverage the same
# sequence-to-sequence transformer model discussed previously and make necessary modifications.
import pdfplumber
import textwrap
from fpdf import FPDF

def translate_pdf(input_file, translator_model,output_file):
    translated_text = ""

    # Read the input PDF file
    with pdfplumber.open(input_file) as pdf:


        # Extract text from each page of the PDF
        for page in pdf.pages:
            text_content = page.extract_text()
            num_pages = len(pdf.pages)
            a4_width_mm = 210
            pt_to_mm = 0.35
            fontsize_pt = 10
            fontsize_mm = fontsize_pt * pt_to_mm
            margin_bottom_mm = 10
            character_width_mm = 7 * pt_to_mm
            width_text = a4_width_mm / character_width_mm

            pdf = FPDF(orientation='P', unit='mm', format='A4')
            pdf.set_auto_page_break(True, margin=margin_bottom_mm)
            pdf.add_page()
            pdf.set_font(family='Courier', size=fontsize_pt)
            # Split the text into sentences
            sentences = text_content.split(".")

            # Translate each sentence using the custom translator model
            for sentence in sentences:
                translated_sentence = translate(translator_model,sentence)
                lines = textwrap.wrap(translated_sentence, width_text)

                if len(lines) == 0:
                    pdf.ln()

                for wrap in lines:
                    pdf.cell(0, fontsize_mm, wrap, ln=1)

            pdf.output(output_file, 'F')


#Here is a German document for you to convert
!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'

#Now call the translate_pdf for the German file as an input to the function and check the output file for the translated file.
input_file_path = "input_de.pdf"
output_file = 'output_en.pdf'
translate_pdf(input_file_path, transformer,output_file)
print("Translated PDF file is saved as:", output_file)